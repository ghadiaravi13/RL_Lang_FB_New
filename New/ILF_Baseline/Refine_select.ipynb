{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce523c-f7e2-459c-89d3-775573e476e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "dataset = load_dataset(\"McGill-NLP/feedbackQA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc95b78-59a0-49a5-aacc-c7a83a28c4b4",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19540bc5-26a6-4065-ad95-0098eef67e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_refined.csv',keep_default_na=False)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910fafe-7d78-4fc9-b29a-f21351a87617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_df1 = train_df.replace('None',None).dropna(axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ced698-54a8-4e30-9105-7286f91b36f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db88edd-dcdd-48c7-bccf-d62173ee1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc39b6f-0635-4fad-b919-47c43474f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"/home/jupyter/Ravi_new/RL_Language_Feedback/New/ILF_Baseline/Llama_Checkpoints\",tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a3e2c-fe44-4b90-8ba8-290328a1235d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from accelerate import Accelerator\n",
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-chat-hf',cache_dir='/home/jupyter/Ravi_new/HF_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4846cf9-26a2-4331-b6ac-47c71774066c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11044ed-76fb-47e6-b4a9-75313341e165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare(model)\n",
    "accelerator.load_state('Llama_Checkpoints/pytorch_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18719e8-99c8-4f3c-8092-7b95821e09c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.get_state_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058dd098-a2b8-4a99-a456-4e89f1b95233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load('Llama_Checkpoints/best_model_chkpt.pth.tar')\n",
    "# model.load_state_dict(ckpt['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323c263-9f93-4005-b6ed-7f98b13a78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt['model_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed50c5e-50b8-40b0-a54a-1dd1f461f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.llm_engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661642bb-b4bd-4093-b6ab-0d1855838d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df1.iloc[150]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2b766-5f71-44f0-a290-39d62982b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/jupyter/Ravi_new/RL_Language_Feedback/New/ILF_Baseline/Llama_Checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036ec9c-ff35-413b-8297-a6485ff36b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SamplingParams(max_tokens=200, best_of=3)\n",
    "\n",
    "prompt = '<s>Question: What can remote and aboriginal communities do help to stay safe?</s>Answer: '\n",
    "prompt_token_ids = tokenizer(prompt,add_special_tokens=False)['input_ids']\n",
    "out = llm.generate(prompt_token_ids=[prompt_token_ids],sampling_params=params)\n",
    "l = [o.text for o in out[0].outputs]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db7fb0-f575-4496-8d5f-8e21edb83eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21036279-af25-4905-b914-7e0e42149563",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1552\n",
    "\n",
    "question = val_df1.loc[i]['question']\n",
    "answer = val_df1.loc[i]['answer']\n",
    "feedback = val_df1.loc[i]['explanation']\n",
    "r2 = val_df1.loc[i]['refined_answer_0'].replace('<unk>','').replace('\\n','')\n",
    "r1 = val_df1.loc[i]['refined_answer_1'].replace('<unk>','').replace('\\n','')\n",
    "r0 = val_df1.loc[i]['refined_answer_2'].replace('<unk>','').replace('\\n','')\n",
    "# r3 = val_df1.loc[i]['refined_answer_3'].replace('<unk>','').replace('\\n','')\n",
    "\n",
    "prompt = f'I will give you a question, an initial answer, a feedback critquing that answer, and 4 different refined answers that try to incorporate the feedback. I want you to tell me, which of the four refined answers best incorporate the feedback. Strictly follow this format: if refined_answer_X is the best, just say, Best Answer: refined_answer_X. Do NOT generate anything else. \\n Question: {question} \\n Answer: {answer} \\n Feedback: {feedback} \\n refined_answer_0: {r0} \\n refined_answer_1: {r1} \\n refined_answer_2: {r2} \\n Best Answer: '\n",
    "\n",
    "params = SamplingParams(max_tokens=200,n=8)\n",
    "out = llm.generate([prompt],sampling_params=params)\n",
    "l = ['refined_answer_0' if 'refined_answer_0' in o.text else ('refined_answer_1' if 'refined_answer_1' in o.text else ('refined_answer_2' if 'refined_answer_2' in o.text else 'NA')) for o in out[0].outputs]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f471470-e8bf-42b0-bc80-ef5efd54ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for i in tqdm_notebook(range(len(val_df1))):\n",
    "    question = val_df1.loc[i]['question']\n",
    "    answer = val_df1.loc[i]['answer']\n",
    "    feedback = val_df1.loc[i]['explanation']\n",
    "    \n",
    "    r0 = val_df1.loc[i]['refined_answer_0'].replace('<unk>','').replace('\\n','')\n",
    "    r1 = val_df1.loc[i]['refined_answer_1'].replace('<unk>','').replace('\\n','')\n",
    "    r2 = val_df1.loc[i]['refined_answer_2'].replace('<unk>','').replace('\\n','')\n",
    "    #r3 = val_df1.loc[i]['refined_answer_3'].replace('<unk>','').replace('\\n','')\n",
    "    \n",
    "    prompt = f'I will give you a question, an initial answer, a feedback critquing that answer, and 4 different refined answers that try to incorporate the feedback. I want you to tell me, which of the four refined answers best incorporate the feedback. Strictly follow this format: if refined_answer_X is the best, just say, Best Answer: refined_answer_X. Do NOT generate anything else. \\n Question: {question} \\n Answer: {answer} \\n Feedback: {feedback} \\n refined_answer_0: {r0} \\n refined_answer_1: {r1} \\n refined_answer_2: {r2} \\n Best Answer: '\n",
    "    \n",
    "    prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ce799-378f-48b3-aa7f-ca4eeef04e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SamplingParams(max_tokens=50,n=8)\n",
    "out = llm.generate(prompts,sampling_params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa82e2-6e8c-46fa-8930-c7b5a440b109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e37cb-a3b2-4c0e-99b8-dff9790b28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.randint(a=0,b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3432d5-622a-46e1-824f-6db406304064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import tqdm\n",
    "\n",
    "val_df1['selected_answer'] = ['None']*len(val_df1)\n",
    "na_selected = 0\n",
    "for i in tqdm.notebook.tqdm(range(len(prompts))):\n",
    "    l = ['refined_answer_0' if 'refined_answer_0' in o.text else ('refined_answer_1' if 'refined_answer_1' in o.text else ('refined_answer_2' if 'refined_answer_2' in o.text else ('refined_answer_3' if 'refined_answer_3' in o.text else 'NA'))) for o in out[i].outputs]\n",
    "    c = Counter(l).most_common(2)\n",
    "\n",
    "    selected = 'NA'\n",
    "    \n",
    "    if c[0][0]=='NA':\n",
    "        try:\n",
    "            selected = c[1][0]\n",
    "        except:\n",
    "            na_selected += 1\n",
    "            continue\n",
    "    else:\n",
    "        selected = c[0][0]\n",
    "        \n",
    "    if selected not in val_df1.columns:\n",
    "        na_selected += 1\n",
    "        selected = f'refined_answer_{random.randint(a=0,b=2)}'\n",
    "    val_df1['selected_answer'].loc[i] = val_df1[selected].loc[i]\n",
    "print(f'NA Selected: {na_selected}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4bc8b-87ca-474d-a229-72b19ee0b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306aaca-78f9-4618-bbc8-9935d4ac0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df1.to_csv('train_selected_refined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c24e83-d575-4192-964d-f76f36ac78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df['rating_class']=='3'][['question','answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b3ba2-50a5-4848-89a5-928c39e48a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([train_df[train_df['rating_class']=='3'][['question','answer']], val_df1[['question','selected_answer']].rename(columns={'selected_answer':'answer'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a7d6a-bdee-4570-a4cc-e32d320baddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2eea6a-601d-440e-b1a6-1adb338e1770",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce0021-70c8-4ad3-ada7-a1e6874ab022",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbadf877-760e-4258-af5e-7c848d30a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda:1')\n",
    "model.eval()\n",
    "\n",
    "i=1394\n",
    "\n",
    "question = val_df1.loc[i]['question']\n",
    "answer = val_df1.loc[i]['answer']\n",
    "feedback = val_df1.loc[i]['explanation']\n",
    "r2 = val_df1.loc[i]['refined_answer_0'].replace('<unk>','').replace('\\n','')\n",
    "r0 = val_df1.loc[i]['refined_answer_1'].replace('<unk>','').replace('\\n','')\n",
    "r1 = val_df1.loc[i]['refined_answer_2'].replace('<unk>','').replace('\\n','')\n",
    "r3 = val_df1.loc[i]['refined_answer_3'].replace('<unk>','').replace('\\n','')\n",
    "\n",
    "prompt = f'I will give you a question, an initial answer, a feedback critquing that answer, and 2 different refined answers that try to incorporate the feedback. I want you to tell me, which of the two refined answers better incorporate the feedback. Strictly follow this format: if refined_answer_X is better, just say, Better Answer: refined_answer_X. \\n Question: {question} \\n Answer: {answer} \\n Feedback: {feedback} \\n refined_answer_0: {r2} \\n refined_answer_1: {r0} \\n Better Answer: '\n",
    "\n",
    "inp = tokenizer(prompt,return_tensors='pt',add_special_tokens=True)['input_ids'].to('cuda:1')\n",
    "with torch.no_grad():\n",
    "    out = model.generate(inp, max_new_tokens=50, do_sample=False, num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d6eb2-5bf0-40a3-a444-bf22ad2773a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e563d3-e2e8-409b-aff4-fceff8b60141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model,valid_DL = accelerator.prepare(model,valid_DL)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "num_return_sequences = 2\n",
    "\n",
    "for i in range(num_return_sequences):\n",
    "    val_df[f'refined_answer_{i}'] = ['None']*len(val_df)\n",
    "with torch.no_grad():\n",
    "    for b in valid_DL:\n",
    "        out = model.generate(inputs=b['input'].to(device),\n",
    "                             attention_mask=b['attention_mask'].to(device),\n",
    "                             max_new_tokens=50,\n",
    "                             num_return_sequences=2,\n",
    "                             do_sample=True\n",
    "                            )\n",
    "        \n",
    "        l = [a.split('Refined answer: ')[1].replace('</s>','') for a in tokenizer.batch_decode(out)]\n",
    "        for i in range(num_return_sequences):\n",
    "            val_df[f'refined_answer_{i}'].loc[b['id'].tolist()] = l[i::num_return_sequences]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dca083-f34f-4766-81f0-6b8ee42c09b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.loc[6][['refined_answer_0','refined_answer_1']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94edb53c-4135-4900-a97b-14ce9700113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0,1,2,3,4,5]\n",
    "l[2::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61cb9b-bfb0-47de-8c0b-d0a605b7a0ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_df = refine(bert_chkpt,val_df)\n",
    "val_df.to_csv('val_refined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c23e4-3ac9-46c4-9f62-8733698f73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d315680-59ec-4f8a-aa6d-c2a5181d95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa0c6b-ebc8-4083-808c-f777cb7039ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fe5d6-b6a8-413d-a09a-f7f205bcd3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(out[2]).replace('</s>',''))#.replace('<unk>',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96c83d-155b-4a45-b5da-966ef050ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524eef71-2aad-4803-892f-01b0a6fa82de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = AutoModel.from_pretrained(bert_chkpt).to(device)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    se = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return F.normalize(se, p=2, dim=1)\n",
    "\n",
    "j = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b in train_DL:\n",
    "        out = mean_pooling( model(input_ids=b['context_w_feedback'].to(device), attention_mask=b['context_w_feedback_attn'].to(device)) , b['feedback_pool_mask'].to(device))\n",
    "        print(out.shape)\n",
    "        print('----------------------------')\n",
    "        j+=1\n",
    "        if j>5:\n",
    "            break\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ed8bc-1d6e-47fa-bfbb-0f6b3b1d97b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[[1,2,3,4,5],[6,7,8,9,0]]])\n",
    "t.repeat(2,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39373ab0-572f-42ec-8bee-d6945dd568bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, model_chkpt, device='cuda:0', inp_dim=768, hidden_dims=None, num_classes=4, use_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.bert_model = AutoModel.from_pretrained(model_chkpt).to(device)\n",
    "        \n",
    "        self.use_norm = use_norm\n",
    "        self.inp_layer = nn.Linear(inp_dim,hidden_dims[0])\n",
    "\n",
    "        hidden_layers = []\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layers.append(nn.Linear(hidden_dims[i],hidden_dims[i+1]))\n",
    "            hidden_layers.append(nn.Dropout(p=0.2))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        self.out_layer = nn.Linear(hidden_dims[-1],num_classes)\n",
    "        \n",
    "    def mean_pooling(self,model_output,attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        se = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return se\n",
    "        \n",
    "    def forward(self, b):\n",
    "        y = self.mean_pooling( self.bert_model(input_ids=b['context_w_feedback'].to(self.device), attention_mask=b['context_w_feedback_attn'].to(self.device)),\n",
    "                               b['feedback_pool_mask'].to(self.device))\n",
    "        if self.use_norm:\n",
    "            y = F.normalize(y,p=2,dim=-1)\n",
    "        y = self.inp_layer(y)\n",
    "        y = F.relu(y)\n",
    "        y = self.layers(y)\n",
    "        y = self.out_layer(y)\n",
    "        \n",
    "        return_dict = {}\n",
    "        \n",
    "        return_dict['logits'] = y\n",
    "        return_dict['class_probs'] = F.softmax(y,dim=-1)\n",
    "        return_dict['CE_loss'] = F.cross_entropy(y,b['rating_class'].to(self.device))\n",
    "        return return_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1fcd9-7fb7-4557-a1ee-023591930420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier,train_dl,valid_dl,epochs,optimizer,PATIENCE=20,save_dir=None):\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    \n",
    "    classifier.train()\n",
    "    \n",
    "    loss_acc = 0\n",
    "    num_batches = 0\n",
    "    total_steps = 0\n",
    "    best_valid_loss = np.inf\n",
    "    patience = PATIENCE\n",
    "    \n",
    "    train_loss_arr,valid_loss_arr = [],[]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    classifier.zero_grad()\n",
    "    \n",
    "    for E in range(epochs):\n",
    "        \n",
    "        num_samples = 0\n",
    "        \n",
    "        for b in train_dl:\n",
    "            \n",
    "            y = classifier(b)\n",
    "                          # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                          # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "            loss = y['CE_loss'] #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_acc += loss.item()\n",
    "        \n",
    "            num_batches += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            train_loss_arr.append(loss_acc/num_batches)\n",
    "\n",
    "            if total_steps%100==0 and total_steps!=0:\n",
    "                print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "            \n",
    "        #print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "        \n",
    "        torch.save({'model_state':classifier.state_dict(),\n",
    "                    'optimizer':optimizer.state_dict(),\n",
    "                    'epoch':E},\n",
    "                    f\"{save_dir}/Epoch_{E}_model_chkpt.pth.tar\")\n",
    "        \n",
    "        valid_loss = validate(classifier,valid_dl)\n",
    "        valid_loss_arr.append(valid_loss/len(valid_dl))\n",
    "        \n",
    "        if valid_loss<best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience = PATIENCE\n",
    "            \n",
    "            torch.save({'model_state':classifier.state_dict(),\n",
    "                        'optimizer':optimizer.state_dict(),\n",
    "                        'epoch':E},\n",
    "                        f\"{save_dir}/best_model_chkpt.pth.tar\")\n",
    "        else:\n",
    "            patience -= 1\n",
    "            print(f\"REDUCING PATIENCE...{patience}\")\n",
    "\n",
    "        if patience<=0:\n",
    "            print(\"RUNNING OUT OF PATIENCE... TERMINATING\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return train_loss_arr,valid_loss_arr\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5931d-65fc-421d-8bf3-4bdb58b8b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(classifier,valid_dl):\n",
    "    \n",
    "    classifier.eval()\n",
    "    valid_loss = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for b in valid_dl:\n",
    "            y = classifier(b)\n",
    "                          # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                          # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "            loss = y['CE_loss'] #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "            valid_loss += loss.item()\n",
    "            num_batches+=1\n",
    "            \n",
    "    print(\"Validation Loss:\",valid_loss/num_batches)\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114ae66-56d4-46de-a66e-c8311faa074f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "EPOCHS = 50\n",
    "FREEZE_BERT = False\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "# MPNet = AutoModel.from_pretrained(bert_chkpt).to(device)\n",
    "classifier_model = classifier(bert_chkpt,device=device,hidden_dims=[768,128], num_classes=4, use_norm=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12405b37-8101-433b-b5c7-6f373b484abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FREEZE_BERT:\n",
    "    classifier_model.load_state_dict(torch.load('Rating_sent_MPNET_chkpts_1/best_model_chkpt.pth.tar')['model_state'])\n",
    "    classifier_model.bert_model.requires_grad_(False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(classifier_model.parameters(),lr=1e-4)\n",
    "\n",
    "save_dir = 'Rating_ctxt_FB_MPNET_chkpts_1'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "train_loss,valid_loss = train(classifier_model,\n",
    "                              train_DL,\n",
    "                              valid_DL,\n",
    "                              EPOCHS,\n",
    "                              optimizer,\n",
    "                              PATIENCE=5,\n",
    "                              save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91101fb6-ccc5-4b70-a781-f80306d79560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train_loss.json','w') as f:\n",
    "    json.dump(train_loss,f)\n",
    "\n",
    "with open('valid_loss.json','w') as f:\n",
    "    json.dump(valid_loss,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1438eb8-e8f1-41bd-ab9c-8c0ee15ab891",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ds = np.array(train_loss)[np.round(np.linspace(0, len(train_loss) - 1, len(valid_loss))).astype(int)]\n",
    "loss_df = pd.DataFrame({'train_loss':train_loss_ds , 'valid_loss':valid_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32161e3-1a96-4893-ac8a-5b567044334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as px\n",
    "px.line(loss_df,y=['train_loss','valid_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00229a87-1ae6-437a-b7fd-b5d9bac756d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DL = DataLoader(test_dataset,batch_size=100,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95aa46-fa36-4e08-9beb-38400ebddeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt = torch.load('Rating_ctxt_FB_MPNET_chkpts_1/best_model_chkpt.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b9d00-0197-4bcc-be96-83e62d9c96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.load_state_dict(chkpt['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3a035-e5a3-49e4-bbf4-6a0699945e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "preds,gt = [],[]\n",
    "classifier_model.eval()\n",
    "with torch.no_grad():\n",
    "    for b in tqdm.tqdm(test_DL,desc='evaluating'):\n",
    "        out = classifier_model(b)\n",
    "        pred_labels = out['class_probs'].argmax(dim=-1).cpu().tolist()\n",
    "        gt_labels = b['rating_class'].tolist()\n",
    "        preds.extend(pred_labels)\n",
    "        gt.extend(gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96357d85-64e1-4d7e-8eab-ee36d405bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix\n",
    "cm = confusion_matrix(gt,preds,normalize='all')\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0bf9b-fa6f-44b1-8c67-19479faf6a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076fa34-e8db-4702-8c36-118ff4185670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "print('Precision: ' , precision_score(gt,preds,average='macro'))\n",
    "print('Recall: ' , recall_score(gt,preds,average='macro'))\n",
    "print('Accuracy: ' , accuracy_score(gt,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107535f6-1c8b-4464-9d54-bede6445dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: ' , precision_score(gt,preds,average='micro'))\n",
    "print('Recall: ' , recall_score(gt,preds,average='micro'))\n",
    "print('Accuracy: ' , accuracy_score(gt,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b00097-4996-4e0b-b608-d3cb30054420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
