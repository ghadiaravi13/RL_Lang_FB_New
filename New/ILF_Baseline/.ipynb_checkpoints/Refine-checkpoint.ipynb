{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce523c-f7e2-459c-89d3-775573e476e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "\n",
    "dataset = load_dataset(\"McGill-NLP/feedbackQA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc95b78-59a0-49a5-aacc-c7a83a28c4b4",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626aded-95e1-47ec-bb47-59914940c8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rating_class = {'Excellent':3 , 'Acceptable':2 , 'Could be Improved':1, 'Bad': 0}\n",
    "\n",
    "def process_df(df):\n",
    "    df['list_feedback'] = df['feedback'].apply(lambda x: [ r + \"___\" + e for r,e in zip(x['rating'],x['explanation']) ])\n",
    "    df['sampled_feedback'] = df['list_feedback'].apply(lambda x: x[0].split(\"___\") if (x[0].split(\"___\")[0]!='Excellent' and x[0].split(\"___\")[0]!='Acceptable') else (x[1].split(\"___\") if (x[1].split(\"___\")[0]!='Excellent' and x[1].split(\"___\")[0]!='Acceptable') else np.random.choice(x).split(\"___\")) )\n",
    "    df['rating_class'] = df['sampled_feedback'].apply(lambda x: rating_class[x[0]])\n",
    "    df['rating'] = df['sampled_feedback'].apply(lambda x: x[0])\n",
    "    df['explanation'] = df['sampled_feedback'].apply(lambda x: x[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ffa3d-fae5-45d9-89c6-899dba3982fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = process_df(pd.DataFrame(dataset['train']))\n",
    "val_df = process_df(pd.DataFrame(dataset['validation']))\n",
    "test_df = process_df(pd.DataFrame(dataset['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5935e0-80ea-4ec8-8e7c-3d7f63a3d1c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "bert_chkpt = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_chkpt,cache_dir='/home/jupyter/Ravi_new/HF_cache')\n",
    "model = AutoModelForCausalLM.from_pretrained(bert_chkpt,cache_dir='/home/jupyter/Ravi_new/HF_cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22efe18-53a9-4974-bb9b-182080220e0b",
   "metadata": {},
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208eaaf3-37eb-4833-94ca-9c5734910132",
   "metadata": {},
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd89391-527e-43b1-a74e-0547dc598993",
   "metadata": {
    "tags": []
   },
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce3579a-84fd-42a7-bcf4-87201558fe11",
   "metadata": {},
   "source": [
    "train_df['answer'].loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858fa93-3ecb-4fd5-909b-0672682e88e3",
   "metadata": {},
   "source": [
    "tokenizer('Hello, how are you doing?'+ f\" {tokenizer.eos_token} \" + \"Hemlooooo\",add_special_tokens=True,return_tensors='pt', return_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89006f7c-adce-48be-8dfb-87ec38529f83",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29210f9c-43b4-4fb0-97e9-feadbf3ed0c9",
   "metadata": {},
   "source": [
    "from nltk import tokenize as nltk_tokenizer\n",
    "len(nltk_tokenizer.sent_tokenize(train_df['answer'].loc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4b69b-5584-4d22-b379-4c004ec95847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "bert_chkpt = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(bert_chkpt,cache_dir='/home/jupyter/Ravi_new/HF_cache')\n",
    "# model = AutoModelForCausalLM.from_pretrained(bert_chkpt,cache_dir='/home/jupyter/Ravi_new/HF_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98fa790-2c79-451e-a477-d26f1696527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class feedback_QA_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,df,tokenizer,max_length=2048):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        skipped = 0\n",
    "        \n",
    "        for i in range(len(self.df)):\n",
    "            \n",
    "            d = {}\n",
    "            if self.df.iloc[i]['rating_class']==3:\n",
    "                skipped += 1\n",
    "                continue\n",
    "                \n",
    "            prompt = \"I will give you a question, an initial answer to the question, and feedback critiquing that answer. Based on the feedback, provide a refined answer. Do NOT generate anything other than the refined answer.\"\n",
    "            question = self.df.iloc[i]['question']\n",
    "            answer = self.df.iloc[i]['answer']\n",
    "            feedback = self.df.iloc[i]['explanation']\n",
    "            \n",
    "            tok_input = self.tokenizer(f\"{prompt}\\nQuestion:{question}\\nAnswer:{answer}\\n\\nFeedback:{feedback}\\n\\nRefined answer: \",\n",
    "                                  add_special_tokens=True\n",
    "                                 )\n",
    "            if len(tok_input['input_ids']) > self.max_len:\n",
    "                skipped += 1\n",
    "                continue\n",
    "                \n",
    "\n",
    "            \n",
    "            PAD_LEN = self.max_len - len(tok_input['input_ids'])\n",
    "\n",
    "            d['input'] = tok_input['input_ids'] + [self.tokenizer.eos_token_id]*PAD_LEN\n",
    "            d['attention_mask'] = tok_input['attention_mask'] + [0]*PAD_LEN\n",
    "            d['id'] = i\n",
    "\n",
    "            for k in d.keys():\n",
    "                d[k] = torch.tensor(d[k])\n",
    "\n",
    "            self.data.append(d)\n",
    "        # print(f'skipped: {skipped}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74550fb6-8a4a-4773-a0fd-45a3cd6c4961",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "train_dataset = feedback_QA_dataset(train_df)\n",
    "valid_dataset = feedback_QA_dataset(val_df)\n",
    "test_dataset = feedback_QA_dataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f4dbc-b8e0-4e41-8045-d93a03b79ce2",
   "metadata": {},
   "source": [
    "train_DL = DataLoader(train_dataset,batch_size=3,shuffle=False)\n",
    "valid_DL = DataLoader(valid_dataset,batch_size=3,shuffle=False)\n",
    "test_DL = DataLoader(test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661223bb-76b9-4f37-9c6d-8a657d8f18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def refine(bert_chkpt,val_df):#,Accelerator):\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "    # accelerator = Accelerator()\n",
    "    device = 'cuda:1'#accelerator.device\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_chkpt,cache_dir='/home/jupyter/Ravi_new/HF_cache')\n",
    "    model = AutoModelForCausalLM.from_pretrained(bert_chkpt,cache_dir='/home/jupyter/Ravi_new/HF_cache')\n",
    "    \n",
    "    train_dataset = feedback_QA_dataset(train_df,tokenizer)\n",
    "    valid_dataset = feedback_QA_dataset(val_df,tokenizer)\n",
    "    test_dataset = feedback_QA_dataset(test_df,tokenizer)\n",
    "\n",
    "    train_DL = DataLoader(train_dataset,batch_size=3,shuffle=False)\n",
    "    valid_DL = DataLoader(valid_dataset,batch_size=3,shuffle=False)\n",
    "    test_DL = DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "    # model,valid_DL = accelerator.prepare(model,valid_DL)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for b in valid_DL:\n",
    "            out = model.generate(inputs=b['input'].to(device),\n",
    "                                 attention_mask=b['attention_mask'].to(device),\n",
    "                                 max_new_tokens=500,\n",
    "                                 num_return_sequences=1,\n",
    "                                 do_sample=True\n",
    "                                )\n",
    "            \n",
    "            l = [a.split('Refined answer: ')[1].replace('</s>','') for a in tokenizer.batch_decode(out)]\n",
    "            val_df['refined_answer'].loc[b['id'].tolist()] = l\n",
    "            # break\n",
    "    return val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a84f1-5705-4dde-aaec-92d3771c58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450e450-b84b-4099-bbda-f6eaebf4558d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_df['refined_answer'] = ['None']*len(val_df)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "# from accelerate import notebook_launcher\n",
    "accelerate.notebook_launcher(refine,(bert_chkpt,val_df,accelerate.Accelerator),num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130696e2-8313-4ef1-9b28-8a794cd544cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'#accelerator.device\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_chkpt,cache_dir='/home/jupyter/Ravi_new/HF_cache')\n",
    "model = AutoModelForCausalLM.from_pretrained(bert_chkpt,cache_dir='/home/jupyter/Ravi_new/HF_cache')\n",
    "\n",
    "train_dataset = feedback_QA_dataset(train_df,tokenizer)\n",
    "valid_dataset = feedback_QA_dataset(val_df,tokenizer)\n",
    "test_dataset = feedback_QA_dataset(test_df,tokenizer)\n",
    "\n",
    "train_DL = DataLoader(train_dataset,batch_size=3,shuffle=False)\n",
    "valid_DL = DataLoader(valid_dataset,batch_size=3,shuffle=False)\n",
    "test_DL = DataLoader(test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e563d3-e2e8-409b-aff4-fceff8b60141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model,valid_DL = accelerator.prepare(model,valid_DL)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "num_return_sequences = 2\n",
    "\n",
    "for i in range(num_return_sequences):\n",
    "    val_df[f'refined_answer_{i}'] = ['None']*len(val_df)\n",
    "with torch.no_grad():\n",
    "    for b in valid_DL:\n",
    "        out = model.generate(inputs=b['input'].to(device),\n",
    "                             attention_mask=b['attention_mask'].to(device),\n",
    "                             max_new_tokens=50,\n",
    "                             num_return_sequences=2,\n",
    "                             do_sample=True\n",
    "                            )\n",
    "        \n",
    "        l = [a.split('Refined answer: ')[1].replace('</s>','') for a in tokenizer.batch_decode(out)]\n",
    "        for i in range(num_return_sequences):\n",
    "            val_df[f'refined_answer_{i}'].loc[b['id'].tolist()] = l[i::num_return_sequences]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dca083-f34f-4766-81f0-6b8ee42c09b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.loc[6][['refined_answer_0','refined_answer_1']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94edb53c-4135-4900-a97b-14ce9700113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0,1,2,3,4,5]\n",
    "l[2::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61cb9b-bfb0-47de-8c0b-d0a605b7a0ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_df = refine(bert_chkpt,val_df)\n",
    "val_df.to_csv('val_refined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c23e4-3ac9-46c4-9f62-8733698f73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d315680-59ec-4f8a-aa6d-c2a5181d95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa0c6b-ebc8-4083-808c-f777cb7039ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fe5d6-b6a8-413d-a09a-f7f205bcd3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(out[2]).replace('</s>',''))#.replace('<unk>',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96c83d-155b-4a45-b5da-966ef050ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524eef71-2aad-4803-892f-01b0a6fa82de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = AutoModel.from_pretrained(bert_chkpt).to(device)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    se = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return F.normalize(se, p=2, dim=1)\n",
    "\n",
    "j = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b in train_DL:\n",
    "        out = mean_pooling( model(input_ids=b['context_w_feedback'].to(device), attention_mask=b['context_w_feedback_attn'].to(device)) , b['feedback_pool_mask'].to(device))\n",
    "        print(out.shape)\n",
    "        print('----------------------------')\n",
    "        j+=1\n",
    "        if j>5:\n",
    "            break\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ed8bc-1d6e-47fa-bfbb-0f6b3b1d97b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[[1,2,3,4,5],[6,7,8,9,0]]])\n",
    "t.repeat(2,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39373ab0-572f-42ec-8bee-d6945dd568bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, model_chkpt, device='cuda:0', inp_dim=768, hidden_dims=None, num_classes=4, use_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.bert_model = AutoModel.from_pretrained(model_chkpt).to(device)\n",
    "        \n",
    "        self.use_norm = use_norm\n",
    "        self.inp_layer = nn.Linear(inp_dim,hidden_dims[0])\n",
    "\n",
    "        hidden_layers = []\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layers.append(nn.Linear(hidden_dims[i],hidden_dims[i+1]))\n",
    "            hidden_layers.append(nn.Dropout(p=0.2))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        self.out_layer = nn.Linear(hidden_dims[-1],num_classes)\n",
    "        \n",
    "    def mean_pooling(self,model_output,attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        se = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return se\n",
    "        \n",
    "    def forward(self, b):\n",
    "        y = self.mean_pooling( self.bert_model(input_ids=b['context_w_feedback'].to(self.device), attention_mask=b['context_w_feedback_attn'].to(self.device)),\n",
    "                               b['feedback_pool_mask'].to(self.device))\n",
    "        if self.use_norm:\n",
    "            y = F.normalize(y,p=2,dim=-1)\n",
    "        y = self.inp_layer(y)\n",
    "        y = F.relu(y)\n",
    "        y = self.layers(y)\n",
    "        y = self.out_layer(y)\n",
    "        \n",
    "        return_dict = {}\n",
    "        \n",
    "        return_dict['logits'] = y\n",
    "        return_dict['class_probs'] = F.softmax(y,dim=-1)\n",
    "        return_dict['CE_loss'] = F.cross_entropy(y,b['rating_class'].to(self.device))\n",
    "        return return_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1fcd9-7fb7-4557-a1ee-023591930420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier,train_dl,valid_dl,epochs,optimizer,PATIENCE=20,save_dir=None):\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    \n",
    "    classifier.train()\n",
    "    \n",
    "    loss_acc = 0\n",
    "    num_batches = 0\n",
    "    total_steps = 0\n",
    "    best_valid_loss = np.inf\n",
    "    patience = PATIENCE\n",
    "    \n",
    "    train_loss_arr,valid_loss_arr = [],[]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    classifier.zero_grad()\n",
    "    \n",
    "    for E in range(epochs):\n",
    "        \n",
    "        num_samples = 0\n",
    "        \n",
    "        for b in train_dl:\n",
    "            \n",
    "            y = classifier(b)\n",
    "                          # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                          # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "            loss = y['CE_loss'] #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_acc += loss.item()\n",
    "        \n",
    "            num_batches += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            train_loss_arr.append(loss_acc/num_batches)\n",
    "\n",
    "            if total_steps%100==0 and total_steps!=0:\n",
    "                print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "            \n",
    "        #print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "        \n",
    "        torch.save({'model_state':classifier.state_dict(),\n",
    "                    'optimizer':optimizer.state_dict(),\n",
    "                    'epoch':E},\n",
    "                    f\"{save_dir}/Epoch_{E}_model_chkpt.pth.tar\")\n",
    "        \n",
    "        valid_loss = validate(classifier,valid_dl)\n",
    "        valid_loss_arr.append(valid_loss/len(valid_dl))\n",
    "        \n",
    "        if valid_loss<best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience = PATIENCE\n",
    "            \n",
    "            torch.save({'model_state':classifier.state_dict(),\n",
    "                        'optimizer':optimizer.state_dict(),\n",
    "                        'epoch':E},\n",
    "                        f\"{save_dir}/best_model_chkpt.pth.tar\")\n",
    "        else:\n",
    "            patience -= 1\n",
    "            print(f\"REDUCING PATIENCE...{patience}\")\n",
    "\n",
    "        if patience<=0:\n",
    "            print(\"RUNNING OUT OF PATIENCE... TERMINATING\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return train_loss_arr,valid_loss_arr\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5931d-65fc-421d-8bf3-4bdb58b8b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(classifier,valid_dl):\n",
    "    \n",
    "    classifier.eval()\n",
    "    valid_loss = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for b in valid_dl:\n",
    "            y = classifier(b)\n",
    "                          # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                          # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "            loss = y['CE_loss'] #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "            valid_loss += loss.item()\n",
    "            num_batches+=1\n",
    "            \n",
    "    print(\"Validation Loss:\",valid_loss/num_batches)\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114ae66-56d4-46de-a66e-c8311faa074f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "EPOCHS = 50\n",
    "FREEZE_BERT = False\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "# MPNet = AutoModel.from_pretrained(bert_chkpt).to(device)\n",
    "classifier_model = classifier(bert_chkpt,device=device,hidden_dims=[768,128], num_classes=4, use_norm=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12405b37-8101-433b-b5c7-6f373b484abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FREEZE_BERT:\n",
    "    classifier_model.load_state_dict(torch.load('Rating_sent_MPNET_chkpts_1/best_model_chkpt.pth.tar')['model_state'])\n",
    "    classifier_model.bert_model.requires_grad_(False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(classifier_model.parameters(),lr=1e-4)\n",
    "\n",
    "save_dir = 'Rating_ctxt_FB_MPNET_chkpts_1'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "train_loss,valid_loss = train(classifier_model,\n",
    "                              train_DL,\n",
    "                              valid_DL,\n",
    "                              EPOCHS,\n",
    "                              optimizer,\n",
    "                              PATIENCE=5,\n",
    "                              save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91101fb6-ccc5-4b70-a781-f80306d79560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train_loss.json','w') as f:\n",
    "    json.dump(train_loss,f)\n",
    "\n",
    "with open('valid_loss.json','w') as f:\n",
    "    json.dump(valid_loss,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1438eb8-e8f1-41bd-ab9c-8c0ee15ab891",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ds = np.array(train_loss)[np.round(np.linspace(0, len(train_loss) - 1, len(valid_loss))).astype(int)]\n",
    "loss_df = pd.DataFrame({'train_loss':train_loss_ds , 'valid_loss':valid_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32161e3-1a96-4893-ac8a-5b567044334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as px\n",
    "px.line(loss_df,y=['train_loss','valid_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00229a87-1ae6-437a-b7fd-b5d9bac756d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DL = DataLoader(test_dataset,batch_size=100,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95aa46-fa36-4e08-9beb-38400ebddeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt = torch.load('Rating_ctxt_FB_MPNET_chkpts_1/best_model_chkpt.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b9d00-0197-4bcc-be96-83e62d9c96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.load_state_dict(chkpt['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3a035-e5a3-49e4-bbf4-6a0699945e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "preds,gt = [],[]\n",
    "classifier_model.eval()\n",
    "with torch.no_grad():\n",
    "    for b in tqdm.tqdm(test_DL,desc='evaluating'):\n",
    "        out = classifier_model(b)\n",
    "        pred_labels = out['class_probs'].argmax(dim=-1).cpu().tolist()\n",
    "        gt_labels = b['rating_class'].tolist()\n",
    "        preds.extend(pred_labels)\n",
    "        gt.extend(gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96357d85-64e1-4d7e-8eab-ee36d405bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix\n",
    "cm = confusion_matrix(gt,preds,normalize='all')\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0bf9b-fa6f-44b1-8c67-19479faf6a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076fa34-e8db-4702-8c36-118ff4185670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "print('Precision: ' , precision_score(gt,preds,average='macro'))\n",
    "print('Recall: ' , recall_score(gt,preds,average='macro'))\n",
    "print('Accuracy: ' , accuracy_score(gt,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107535f6-1c8b-4464-9d54-bede6445dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: ' , precision_score(gt,preds,average='micro'))\n",
    "print('Recall: ' , recall_score(gt,preds,average='micro'))\n",
    "print('Accuracy: ' , accuracy_score(gt,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b00097-4996-4e0b-b608-d3cb30054420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
