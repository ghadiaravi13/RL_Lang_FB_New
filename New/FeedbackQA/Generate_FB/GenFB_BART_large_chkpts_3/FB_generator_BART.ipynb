{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccce523c-f7e2-459c-89d3-775573e476e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset feedback_qa (/home/raja/.cache/huggingface/datasets/McGill-NLP___feedback_qa/plain_text/1.0.0/20c8f938f417c88303bb7041cea9554c1d14667686d7d7c5dda83dd4f39e5dc4)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022179603576660156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 43,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdc4c04e587457f9f5f7357007170a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"McGill-NLP/feedbackQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4626aded-95e1-47ec-bb47-59914940c8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rating_scores = {'Excellent':3 , 'Acceptable':2 , 'Could be Improved':1, 'Bad': -1}\n",
    "\n",
    "def process_df(df):\n",
    "    df['list_feedback'] = df['feedback'].apply(lambda x: [ r + \"___\" + e for r,e in zip(x['rating'],x['explanation']) ])\n",
    "    df['sampled_feedback'] = df['list_feedback'].apply(lambda x: np.random.choice(x).split(\"___\") )\n",
    "    df['rating_score'] = df['sampled_feedback'].apply(lambda x: rating_scores[x[0]])\n",
    "    df['rating'] = df['sampled_feedback'].apply(lambda x: x[0])\n",
    "    df['explanation'] = df['sampled_feedback'].apply(lambda x: x[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd2ffa3d-fae5-45d9-89c6-899dba3982fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = process_df(pd.DataFrame(dataset['train']))\n",
    "val_df = process_df(pd.DataFrame(dataset['validation']))\n",
    "test_df = process_df(pd.DataFrame(dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40dc46db-a864-459a-8e7a-b82df484643b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "bart_chkpt = 'facebook/bart-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(bart_chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744c4863-8096-4c67-b473-90e03a0df39c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>feedback</th>\n",
       "      <th>list_feedback</th>\n",
       "      <th>sampled_feedback</th>\n",
       "      <th>rating_score</th>\n",
       "      <th>rating</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>Coronavirus (COVID-19) information for job see...</td>\n",
       "      <td>{'rating': ['Excellent', 'Could be Improved'],...</td>\n",
       "      <td>[Excellent___Has a link to detailed informatio...</td>\n",
       "      <td>[Could be Improved, This answer provides a lin...</td>\n",
       "      <td>1</td>\n",
       "      <td>Could be Improved</td>\n",
       "      <td>This answer provides a link for job searches, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>Coronavirus (COVID-19) information for job see...</td>\n",
       "      <td>{'rating': ['Excellent', 'Excellent'], 'explan...</td>\n",
       "      <td>[Excellent___A link to a job search website is...</td>\n",
       "      <td>[Excellent, Includes a link to a Jobs Hub page...</td>\n",
       "      <td>3</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Includes a link to a Jobs Hub page, which is b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>Coronavirus (COVID-19) information and support...</td>\n",
       "      <td>{'rating': ['Bad', 'Acceptable'], 'explanation...</td>\n",
       "      <td>[Bad___Talks about tax credits for businesses ...</td>\n",
       "      <td>[Acceptable, This answer discusses the Employm...</td>\n",
       "      <td>2</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>This answer discusses the Employment Fund, whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If I am in Australia on a worker holiday marke...</td>\n",
       "      <td>Frequently Asked Questions\\nWorking holiday ma...</td>\n",
       "      <td>{'rating': ['Could be Improved', 'Acceptable']...</td>\n",
       "      <td>[Could be Improved___Answer is about Working H...</td>\n",
       "      <td>[Could be Improved, Answer is about Working Ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>Could be Improved</td>\n",
       "      <td>Answer is about Working Holiday Makers, but do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If I am in Australia on a worker holiday marke...</td>\n",
       "      <td>Frequently Asked Questions\\nCOVID-19 Pandemic ...</td>\n",
       "      <td>{'rating': ['Bad', 'Could be Improved'], 'expl...</td>\n",
       "      <td>[Bad___Discusses pandemic visas. Doesn't menti...</td>\n",
       "      <td>[Bad, Discusses pandemic visas. Doesn't mentio...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Discusses pandemic visas. Doesn't mention the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                   How do I get help finding a job?   \n",
       "1                   How do I get help finding a job?   \n",
       "2                   How do I get help finding a job?   \n",
       "3  If I am in Australia on a worker holiday marke...   \n",
       "4  If I am in Australia on a worker holiday marke...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Coronavirus (COVID-19) information for job see...   \n",
       "1  Coronavirus (COVID-19) information for job see...   \n",
       "2  Coronavirus (COVID-19) information and support...   \n",
       "3  Frequently Asked Questions\\nWorking holiday ma...   \n",
       "4  Frequently Asked Questions\\nCOVID-19 Pandemic ...   \n",
       "\n",
       "                                            feedback  \\\n",
       "0  {'rating': ['Excellent', 'Could be Improved'],...   \n",
       "1  {'rating': ['Excellent', 'Excellent'], 'explan...   \n",
       "2  {'rating': ['Bad', 'Acceptable'], 'explanation...   \n",
       "3  {'rating': ['Could be Improved', 'Acceptable']...   \n",
       "4  {'rating': ['Bad', 'Could be Improved'], 'expl...   \n",
       "\n",
       "                                       list_feedback  \\\n",
       "0  [Excellent___Has a link to detailed informatio...   \n",
       "1  [Excellent___A link to a job search website is...   \n",
       "2  [Bad___Talks about tax credits for businesses ...   \n",
       "3  [Could be Improved___Answer is about Working H...   \n",
       "4  [Bad___Discusses pandemic visas. Doesn't menti...   \n",
       "\n",
       "                                    sampled_feedback  rating_score  \\\n",
       "0  [Could be Improved, This answer provides a lin...             1   \n",
       "1  [Excellent, Includes a link to a Jobs Hub page...             3   \n",
       "2  [Acceptable, This answer discusses the Employm...             2   \n",
       "3  [Could be Improved, Answer is about Working Ho...             1   \n",
       "4  [Bad, Discusses pandemic visas. Doesn't mentio...            -1   \n",
       "\n",
       "              rating                                        explanation  \n",
       "0  Could be Improved  This answer provides a link for job searches, ...  \n",
       "1          Excellent  Includes a link to a Jobs Hub page, which is b...  \n",
       "2         Acceptable  This answer discusses the Employment Fund, whi...  \n",
       "3  Could be Improved  Answer is about Working Holiday Makers, but do...  \n",
       "4                Bad  Discusses pandemic visas. Doesn't mention the ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619de540-d154-431a-8d5e-bab3bba6d7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 31414,     6,   141,    32,    47,   608,   116,  1437,     2,\n",
       "         10869,   462, 40386,   139,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'length': tensor([15])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Hello, how are you doing?'+ f\" {tokenizer.eos_token} \" + \"Hemlooooo\",add_special_tokens=True,return_tensors='pt', return_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a093a991-7536-4082-9bd2-7602825edfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class feedback_QA_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,df,max_length=300):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        for i in tqdm.tqdm(range(len(self.df)),desc='vectorizing..'):\n",
    "            \n",
    "            d = {}\n",
    "            \n",
    "            tok_input = tokenizer(('Question:' + self.df.iloc[i]['question'] + ' Answer: ' + self.df.iloc[i]['answer']).replace(\"\\n\",\" \"),\n",
    "                                  return_token_type_ids=True, \n",
    "                                  add_special_tokens=True, \n",
    "                                  return_length=True,\n",
    "                                  max_length=self.max_len,\n",
    "                                  padding='max_length',\n",
    "                                  truncation='only_first',\n",
    "                                  return_tensors='pt')\n",
    "\n",
    "            feedback = self.df.iloc[i]['explanation']\n",
    "\n",
    "            tok_feedback = tokenizer(tokenizer.pad_token + \"Summary: \" + feedback + tokenizer.eos_token, \n",
    "                                     return_token_type_ids=True,\n",
    "                                     add_special_tokens=False,\n",
    "                                     return_length=True,\n",
    "                                     max_length=300, \n",
    "                                     padding='max_length', \n",
    "                                     truncation='only_first',\n",
    "                                     return_tensors='pt')\n",
    "\n",
    "            d['input'] = tok_input['input_ids'].squeeze(0)\n",
    "            d['input_attn'] = tok_input['attention_mask'].squeeze(0)\n",
    "            d['feedback'] = tok_feedback['input_ids'].squeeze(0)[:-1]\n",
    "            d['feedback_attn'] = tok_feedback['attention_mask'].squeeze(0)[:-1]\n",
    "            \n",
    "            labels = tok_feedback['input_ids'].squeeze(0)[1:].clone()\n",
    "            labels[labels==tokenizer.pad_token_id] = -100\n",
    "            d['labels'] = labels\n",
    "            \n",
    "            d['feedback_len'] = tok_feedback['length'][0]\n",
    "\n",
    "            self.data.append(d)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c69c540d-1963-419c-8c09-08f5d65aa803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vectorizing..: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5660/5660 [00:11<00:00, 474.98it/s]\n",
      "vectorizing..: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1410/1410 [00:02<00:00, 478.87it/s]\n",
      "vectorizing..: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1995/1995 [00:04<00:00, 462.34it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = feedback_QA_dataset(train_df)\n",
    "valid_dataset = feedback_QA_dataset(val_df)\n",
    "test_dataset = feedback_QA_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92049336-0c27-4e1d-b5f7-9791745c5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DL = DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "valid_DL = DataLoader(valid_dataset,batch_size=16,shuffle=True)\n",
    "test_DL = DataLoader(test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1da63016-24f0-400e-9a29-9093ab832bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 45641,    35,  ...,     1,     1,     1],\n",
      "        [    0, 45641,    35,  ...,     1,     1,     1],\n",
      "        [    0, 45641,    35,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 45641,    35,  ...,    49,  4722,     2],\n",
      "        [    0, 45641,    35,  ...,     1,     1,     1],\n",
      "        [    0, 45641,    35,  ...,     1,     1,     1]]) tensor([[47977,    35, 31652,  ...,  -100,  -100,  -100],\n",
      "        [47977,    35,   152,  ...,  -100,  -100,  -100],\n",
      "        [47977,    35,   152,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [47977,    35,   152,  ...,  -100,  -100,  -100],\n",
      "        [47977,    35,    85,  ...,  -100,  -100,  -100],\n",
      "        [47977,    35,   152,  ...,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "for b in train_DL:\n",
    "    print(b['input'] , b['labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b77a0-83a9-4789-868b-4d6b17bc60ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_chkpt).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b in train_DL:\n",
    "        output = bart_model(input_ids=b['input'].to(device),\n",
    "                            decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),)\n",
    "                            #labels=b['feedback'].squeeze(1)[:,1:].to(device))\n",
    "        print(tokenizer.decode(b['input'][:,0][0],skip_special_tokens=True),\"\\n\\n\")\n",
    "        print(tokenizer.decode(b['feedback'][:,0,:-1][0]))\n",
    "        print(tokenizer.decode(b['feedback'][:,0,1:][0]))\n",
    "        print(output.logits.shape)\n",
    "        break\n",
    "\n",
    "del bart_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97a1fcd9-7fb7-4557-a1ee-023591930420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator,train_dl,valid_dl,epochs,optimizer,PATIENCE=20,save_dir=None):\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    \n",
    "    generator.train()\n",
    "    \n",
    "    loss_acc = 0\n",
    "    num_batches = 0\n",
    "    total_steps = 0\n",
    "    best_valid_loss = np.inf\n",
    "    patience = PATIENCE\n",
    "    \n",
    "    train_loss_arr,valid_loss_arr = [],[]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    generator.zero_grad()\n",
    "    \n",
    "    for E in range(epochs):\n",
    "        \n",
    "        for b in train_dl:\n",
    "            \n",
    "            y = generator(input_ids=b['input'].to(device),\n",
    "                          attention_mask=b['input_attn'].to(device),\n",
    "                          decoder_input_ids=b['feedback'].to(device),\n",
    "                          decoder_attention_mask=b['feedback_attn'].to(device),\n",
    "                          labels = b['labels'].to(device))\n",
    "                          # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                          # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "            loss = y.loss #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_acc += loss.item()\n",
    "            num_batches += 1\n",
    "            total_steps += 1\n",
    "            \n",
    "            train_loss_arr.append(loss_acc/num_batches)\n",
    "            \n",
    "            if total_steps%100==0:\n",
    "                print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "            \n",
    "        #print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "        \n",
    "        torch.save({'model_state':generator.state_dict(),\n",
    "                    'optimizer':optimizer.state_dict(),\n",
    "                    'epoch':E},\n",
    "                    f\"{save_dir}/Epoch_{E}_model_chkpt.pth.tar\")\n",
    "        \n",
    "        valid_loss = validate(generator,valid_dl)\n",
    "        valid_loss_arr.append(valid_loss/len(valid_dl))\n",
    "        \n",
    "        if valid_loss<best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience = PATIENCE\n",
    "            \n",
    "            torch.save({'model_state':generator.state_dict(),\n",
    "                        'optimizer':optimizer.state_dict(),\n",
    "                        'epoch':E},\n",
    "                        f\"{save_dir}/best_model_chkpt.pth.tar\")\n",
    "        else:\n",
    "            patience -= 1\n",
    "            print(f\"REDUCING PATIENCE...{patience}\")\n",
    "\n",
    "        if patience<=0:\n",
    "            print(\"RUNNING OUT OF PATIENCE... TERMINATING\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return train_loss_arr,valid_loss_arr\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5e5931d-65fc-421d-8bf3-4bdb58b8b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(generator,valid_dl):\n",
    "    \n",
    "    generator.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for b in valid_dl:\n",
    "            y = generator(input_ids=b['input'].to(device),\n",
    "                          attention_mask=b['input_attn'].to(device),\n",
    "                          decoder_input_ids=b['feedback'].to(device),\n",
    "                          decoder_attention_mask=b['feedback_attn'].to(device),\n",
    "                          labels = b['labels'].to(device))\n",
    "                          # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                          # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "            loss = y.loss #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    print(\"Validation Loss:\",valid_loss)\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1cb84b-dd2e-495f-845d-03ec726b7bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Steps taken: 100 \tLoss: 2.673273162841797\n",
      "Epoch: 0 \t Steps taken: 200 \tLoss: 2.514744558930397\n",
      "Epoch: 0 \t Steps taken: 300 \tLoss: 2.478877408504486\n",
      "Validation Loss: 529.0194516181946\n",
      "Epoch: 1 \t Steps taken: 400 \tLoss: 3.3485342407226564\n",
      "Epoch: 1 \t Steps taken: 500 \tLoss: 3.8570037784576416\n",
      "Epoch: 1 \t Steps taken: 600 \tLoss: 4.190649787584941\n",
      "Epoch: 1 \t Steps taken: 700 \tLoss: 4.424836439405169\n",
      "Validation Loss: 518.0795140266418\n",
      "Epoch: 2 \t Steps taken: 800 \tLoss: 4.5951131355762485\n",
      "Epoch: 2 \t Steps taken: 900 \tLoss: 4.727638207011752\n",
      "Epoch: 2 \t Steps taken: 1000 \tLoss: 4.8276005525588985\n",
      "Validation Loss: 494.9659824371338\n",
      "Epoch: 3 \t Steps taken: 1100 \tLoss: 4.888780029903758\n",
      "Epoch: 3 \t Steps taken: 1200 \tLoss: 4.937873899141947\n",
      "Epoch: 3 \t Steps taken: 1300 \tLoss: 4.9761647052031295\n",
      "Epoch: 3 \t Steps taken: 1400 \tLoss: 5.006569131101881\n",
      "Validation Loss: 486.38301277160645\n",
      "Epoch: 4 \t Steps taken: 1500 \tLoss: 5.026409145991008\n",
      "Epoch: 4 \t Steps taken: 1600 \tLoss: 5.044998742938041\n",
      "Epoch: 4 \t Steps taken: 1700 \tLoss: 5.060106926805833\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "generator = BartForConditionalGeneration.from_pretrained(bart_chkpt).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(generator.parameters(),lr=5e-5)\n",
    "\n",
    "save_dir = 'GenFB_BART_large_chkpts_3'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "train_loss,valid_loss = train(generator,train_DL,valid_DL,50,optimizer,PATIENCE=5,save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91101fb6-ccc5-4b70-a781-f80306d79560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train_loss.json','w') as f:\n",
    "    json.dump(train_loss,f)\n",
    "\n",
    "with open('valid_loss.json','w') as f:\n",
    "    json.dump(valid_loss,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1438eb8-e8f1-41bd-ab9c-8c0ee15ab891",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ds = np.array(train_loss)[np.round(np.linspace(0, len(train_loss) - 1, len(valid_loss))).astype(int)]\n",
    "loss_df = pd.DataFrame({'train_loss':train_loss_ds , 'valid_loss':valid_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32161e3-1a96-4893-ac8a-5b567044334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as px\n",
    "px.line(loss_df,y=['train_loss','valid_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b9d00-0197-4bcc-be96-83e62d9c96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_state_dict(torch.load('GenFB_BART_chkpts_1/Epoch_0_model_chkpt.pth.tar')['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3a035-e5a3-49e4-bbf4-6a0699945e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for b in train_DL:\n",
    "    out = generator.generate(inputs=b['input'][0:1,0].to(device),top_p=0.5)\n",
    "    print(tokenizer.decode(b['input'][0:1,0][0],skip_special_tokens=True))\n",
    "    print(tokenizer.decode(b['feedback'][0:1,0][0],skip_special_tokens=True))\n",
    "    print(tokenizer.decode(out[0]))\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    i+=1\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96357d85-64e1-4d7e-8eab-ee36d405bbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
