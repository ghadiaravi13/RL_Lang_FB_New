{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce523c-f7e2-459c-89d3-775573e476e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "\n",
    "from torch import nn\n",
    "from nltk import tokenize as nltk_tokenizer\n",
    "\n",
    "dataset = load_dataset(\"McGill-NLP/feedbackQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626aded-95e1-47ec-bb47-59914940c8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rating_scores = {'Excellent':3 , 'Acceptable':2 , 'Could be Improved':1, 'Bad': -1}\n",
    "\n",
    "def process_df(df):\n",
    "    df['question'] = df['question'].apply(lambda x: x.replace('\\n',' '))\n",
    "    df['answer'] = df['answer'].apply(lambda x: x.replace('\\n',' '))\n",
    "    df['list_feedback'] = df['feedback'].apply(lambda x: [ r + \"___\" + e for r,e in zip(x['rating'],x['explanation']) ])\n",
    "    df['sampled_feedback'] = df['list_feedback'].apply(lambda x: np.random.choice(x).split(\"___\") )\n",
    "    df['rating_score'] = df['sampled_feedback'].apply(lambda x: rating_scores[x[0]])\n",
    "    df['rating'] = df['sampled_feedback'].apply(lambda x: x[0])\n",
    "    df['explanation'] = df['sampled_feedback'].apply(lambda x: x[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ffa3d-fae5-45d9-89c6-899dba3982fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = process_df(pd.DataFrame(dataset['train']))\n",
    "val_df = process_df(pd.DataFrame(dataset['validation']))\n",
    "test_df = process_df(pd.DataFrame(dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc46db-a864-459a-8e7a-b82df484643b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "bert_chkpt = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_chkpt)\n",
    "# model = AutoModel.from_pretrained(bert_chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c623b-2b7b-4b28-a499-b63ba8311ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c4863-8096-4c67-b473-90e03a0df39c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078f4a5-10f0-4d44-886e-53ae3826ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['answer'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619de540-d154-431a-8d5e-bab3bba6d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('Hello, how are you doing?'+ f\" {tokenizer.eos_token} \" + \"Hemlooooo\",add_special_tokens=True,return_tensors='pt', return_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af181ac2-b1ae-4fcd-b59d-74b4defb3d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nltk_tokenizer.sent_tokenize(train_df['answer'].loc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22dd1b8-d28f-4d16-9acc-cd3dce4579cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tok_inp = tokenizer(nltk_tokenizer.sent_tokenize(train_df['answer'].loc[0]),add_special_tokens=False,return_token_type_ids=True)#,max_length=200,padding='max_length')\n",
    "tok_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6d281-2513-459b-a854-3f099d26309b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = 0\n",
    "tokenizer('Hello, how are you?',return_special_tokens_mask=True,add_special_tokens=True, padding='max_length', max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093a991-7536-4082-9bd2-7602825edfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class feedback_QA_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,df,max_length=512):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.data = []\n",
    "        skipped = 0\n",
    "        \n",
    "        for i in tqdm.tqdm(range(len(self.df)),desc='vectorizing..'):\n",
    "            \n",
    "            d = {'id':i}\n",
    "            \n",
    "            tok_question = tokenizer('Question: ' + self.df.iloc[i]['question'] + ' Answer: ', add_special_tokens=False)\n",
    "            tok_answer = tokenizer(self.df.iloc[i]['answer'].strip().replace('  ',' '), add_special_tokens=False)\n",
    "            tok_feedback = tokenizer(self.df.iloc[i]['explanation'], add_special_tokens=False)\n",
    "\n",
    "            d['question'] = tok_question['input_ids']\n",
    "            d['answer'] = tok_answer['input_ids']\n",
    "            d['feedback'] = tok_feedback['input_ids']\n",
    "            \n",
    "            if len(tok_question['input_ids']+tok_answer['input_ids']+tok_feedback['input_ids'])+4 > self.max_len:\n",
    "                skipped +=1\n",
    "                continue\n",
    "            \n",
    "            context = [tokenizer.bos_token_id] + tok_question['input_ids'] + tok_answer['input_ids']\n",
    "            context_attn = [1] + tok_question['attention_mask'] + tok_answer['attention_mask']\n",
    "            context_pool_mask = [0] + [0]*len(tok_question['input_ids']) + tok_answer['attention_mask']\n",
    "            \n",
    "            \n",
    "            d['context_w_feedback'] = context + [tokenizer.eos_token_id] + tok_feedback['input_ids'] + [tokenizer.eos_token_id]\n",
    "            \n",
    "            PAD_LEN = self.max_len - len(d['context_w_feedback'])\n",
    "\n",
    "            d['Input_len'] = len(d['context_w_feedback'])\n",
    "            d['PAD_LEN'] = PAD_LEN\n",
    "            \n",
    "            d['context_w_feedback'] += [tokenizer.pad_token_id]*PAD_LEN\n",
    "            d['context_w_feedback_attn'] = context_attn + [1] + tok_feedback['attention_mask'] + [1] + [0]*PAD_LEN            \n",
    "            d['context'] = d['context_w_feedback']\n",
    "            d['context_attn'] = context_attn + [1] + [0]*len(tok_feedback['attention_mask']) + [0] + [0]*PAD_LEN\n",
    "            \n",
    "            d['feedback_pool_mask'] = [0]*len(context_pool_mask) + [0] + tok_feedback['attention_mask'] + [0] + [0]*PAD_LEN\n",
    "            d['answer_pool_mask'] = context_pool_mask + [0] + [0]*len(tok_feedback['attention_mask']) + [0] + [0]*PAD_LEN\n",
    "            \n",
    "            answer_phrases = nltk_tokenizer.sent_tokenize(self.df.iloc[i]['answer'].strip().replace('  ',' '))\n",
    "            tok_phrases = tokenizer(answer_phrases,add_special_tokens=False,return_token_type_ids=True)\n",
    "\n",
    "            d['tok_phrases'] = tok_phrases['input_ids']\n",
    "            d['answer_phrases_pool_mask'] = []\n",
    "            \n",
    "            for j in range(len(answer_phrases)):\n",
    "                answer_phrases_attn_mask = tok_phrases['token_type_ids'].copy()\n",
    "                answer_phrases_attn_mask[j] = tok_phrases['attention_mask'][j].copy()\n",
    "                answer_phrases_attn_mask = list(itertools.chain.from_iterable(answer_phrases_attn_mask))\n",
    "                pad_len = len(tok_answer['attention_mask']) - len(answer_phrases_attn_mask)\n",
    "                answer_phrases_attn_mask += [0]*pad_len\n",
    "                \n",
    "                answer_phrase_pool_mask = [0] + [0]*len(tok_question['input_ids']) + answer_phrases_attn_mask + [0] + [0]*len(tok_feedback['attention_mask']) + [0] + [0]*PAD_LEN\n",
    "                \n",
    "                d['answer_phrases_pool_mask'].append(answer_phrase_pool_mask)\n",
    "            \n",
    "            if len(d['answer_phrases_pool_mask'][0])>len(d['answer_pool_mask']):\n",
    "                skipped +=1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                self.data.append(d)\n",
    "                \n",
    "        print('Skipped: ',skipped)\n",
    "\n",
    "    def add_neg_samples(self):\n",
    "        for i in tqdm.tqdm(range(self.__len__()),desc='adding neg samples...'):\n",
    "            self.data[i]['feedback_set'] = [self.data[i]['context_w_feedback']]\n",
    "            self.data[i]['feedback_attn_set'] = [self.data[i]['context_w_feedback_attn']]\n",
    "            self.data[i]['feedback_pool_mask_set'] = [self.data[i]['feedback_pool_mask']]\n",
    "            L = list(range(self.__len__()))\n",
    "            L.remove(i)\n",
    "            neg_samples_idx = np.random.choice(L,size=4)\n",
    "            for n_id in neg_samples_idx:\n",
    "                self.data[i]['feedback_set'].append(self.data[n_id]['context_w_feedback'])\n",
    "                self.data[i]['feedback_attn_set'].append(self.data[n_id]['context_w_feedback_attn'])\n",
    "                self.data[i]['feedback_pool_mask_set'].append(self.data[n_id]['feedback_pool_mask'])\n",
    "            for k in self.data[i].keys():\n",
    "                if k!='tok_phrases':\n",
    "                    self.data[i][k] = torch.tensor(self.data[i][k])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c540d-1963-419c-8c09-08f5d65aa803",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = feedback_QA_dataset(train_df)\n",
    "train_dataset.add_neg_samples()\n",
    "valid_dataset = feedback_QA_dataset(val_df)\n",
    "valid_dataset.add_neg_samples()\n",
    "test_dataset = feedback_QA_dataset(test_df)\n",
    "test_dataset.add_neg_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92049336-0c27-4e1d-b5f7-9791745c5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DL = DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "valid_DL = DataLoader(valid_dataset,batch_size=1,shuffle=True)\n",
    "test_DL = DataLoader(test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da63016-24f0-400e-9a29-9093ab832bbe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for b in train_DL:\n",
    "    for k in b.keys():\n",
    "        print(k)\n",
    "        print(b[k].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4587b-45e7-40cb-962d-faec921133ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[[1,2,3],[4,5,6]]])\n",
    "t.shape,t.repeat(2,1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524eef71-2aad-4803-892f-01b0a6fa82de",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "model = AutoModel.from_pretrained(bert_chkpt,cache_dir='/home/jupyter/Ravi_new/HF_cache').to(device)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    se = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return F.normalize(se, p=2, dim=1)\n",
    "\n",
    "j = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b in test_DL:\n",
    "        se = mean_pooling( model(input_ids = b['context_w_feedback'].to(device),attention_mask=b['context_attn'].to(device))[0], b['answer_pool_mask'].to(device))\n",
    "        \n",
    "        fmo = model(input_ids = b['feedback_set'][0].to(device),attention_mask=b['feedback_attn_set'][0].to(device))[0]\n",
    "        # print(b['feedback_set'][0].shape, b['feedback_attn_set'][0].shape, fmo.shape, b['feedback_pool_mask_set'][0].shape)\n",
    "        \n",
    "        fe = mean_pooling(model(input_ids = b['feedback_set'][0].to(device),attention_mask=b['feedback_attn_set'][0].to(device))[0], b['feedback_pool_mask_set'][0].to(device))\n",
    "        \n",
    "        pmo = model(input_ids = b['context_w_feedback'].to(device),attention_mask=b['context_attn'].to(device))\n",
    "        # print(pmo[0].shape,b['answer_phrases_pool_mask'].shape,pmo[0].repeat(b['answer_phrases_pool_mask'][0].shape[0],1,1).shape,b['answer_phrases_pool_mask'][0].shape)\n",
    "        # print(b['question'].shape,b['answer'].shape,b['feedback'].shape,b['PAD_LEN'],b['answer_pool_mask'].shape,b['id'])\n",
    "        pe = mean_pooling(pmo[0].repeat(b['answer_phrases_pool_mask'][0].shape[0],1,1),b['answer_phrases_pool_mask'][0].to(device) )# for i in range(b['answer_phrases_pool_mask'][0].shape[0])]\n",
    "        # pe = torch.stack(pe).squeeze(1)\n",
    "        cos_sim = F.cosine_similarity(se,fe,dim=1)\n",
    "        cos_phrase_sim = torch.matmul(pe,fe.transpose(1,0))\n",
    "        print(fe.shape,se.shape,pe.shape,cos_sim,cos_phrase_sim.mean(0))\n",
    "        \n",
    "        sent_probs = F.softmax(cos_sim,dim=-1)\n",
    "        phrase_probs = F.softmax(cos_phrase_sim,dim=-1)\n",
    "        \n",
    "        print('\\nInput: ',tokenizer.decode(torch.mul(b['context_w_feedback'][0],b['context_attn'][0]),skip_special_tokens=True),'\\n')\n",
    "        print('Feedback: ',tokenizer.decode(torch.mul(b['context_w_feedback'][0],b['feedback_pool_mask'][0]),skip_special_tokens=True),'\\n')\n",
    "        for i in range(b['answer_phrases_pool_mask'][0].shape[0]):\n",
    "            relevance = phrase_probs[i][0] - sent_probs[0]\n",
    "            \n",
    "            phrase_tok = torch.mul(b['context_w_feedback'][0],b['answer_phrases_pool_mask'][0][i])\n",
    "            print(f\"Phrase {i}:\",tokenizer.decode(phrase_tok,skip_special_tokens=True))\n",
    "            print(f\"Relevance of phrase {i} is {relevance}\",'\\n')\n",
    "        \n",
    "        print('softmax: ',F.softmax(cos_sim),F.softmax(cos_phrase_sim,dim=-1))\n",
    "        \n",
    "        tgt_tensor = torch.zeros(b['feedback_set'].shape[1] , device=device)\n",
    "        tgt_tensor[0] = 1.0\n",
    "        print('CE Loss: ', F.cross_entropy(cos_sim,target=tgt_tensor), F.cross_entropy(cos_phrase_sim.mean(0),target=torch.tensor([1.0,0,0,0,0]).to(device)))\n",
    "        print('----------------------------')\n",
    "        j+=1\n",
    "        if j>15:\n",
    "            break\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ed8bc-1d6e-47fa-bfbb-0f6b3b1d97b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[[1,2,3,4,5],[6,7,8,9,0]]])\n",
    "t.repeat(2,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39373ab0-572f-42ec-8bee-d6945dd568bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    def __init__(self, model_chkpt, device='cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = AutoModel.from_pretrained(bert_chkpt,cache_dir='/home/jupyter/Ravi_new/HF_cache')\n",
    "        self.device = device\n",
    "        \n",
    "    def mean_pooling(self,model_output,attention_mask):\n",
    "        token_embeddings = model_output #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        se = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return F.normalize(se, p=2, dim=1)\n",
    "        \n",
    "    def forward(self, b):\n",
    "        sent_model_out = self.model(input_ids = b['context_w_feedback'].to(self.device),attention_mask=b['context_attn'].to(self.device))[0]\n",
    "        feedback_model_out = self.model(input_ids = b['feedback_set'][0].to(self.device),attention_mask=b['feedback_attn_set'][0].to(self.device))[0]\n",
    "        \n",
    "        sent_emb = self.mean_pooling( sent_model_out, b['answer_pool_mask'].to(self.device))\n",
    "        feedback_emb = self.mean_pooling( feedback_model_out, b['feedback_pool_mask_set'][0].to(self.device))\n",
    "        \n",
    "        # print(pmo[0].shape,b['answer_phrases_pool_mask'].shape)\n",
    "        phrase_emb = self.mean_pooling( sent_model_out.repeat(b['answer_phrases_pool_mask'][0].shape[0],1,1), b['answer_phrases_pool_mask'][0].to(self.device) )\n",
    "        # phrase_emb = torch.stack(phrase_emb).squeeze(1)\n",
    "        cos_sim = F.cosine_similarity(sent_emb,feedback_emb,dim=1)\n",
    "        cos_phrase_sim = torch.matmul(phrase_emb,feedback_emb.transpose(1,0))\n",
    "        \n",
    "        tgt_tensor = torch.zeros(b['feedback_set'].shape[1] , device=self.device)\n",
    "        tgt_tensor[0] = 1.0 #the relevant feedback is always present at index 0\n",
    "        \n",
    "        return_dict = {'sent_ce_loss': F.cross_entropy(cos_sim,target=tgt_tensor),\n",
    "                       'avg_phrase_ce_loss': F.cross_entropy(cos_phrase_sim.mean(0),target=tgt_tensor),\n",
    "                       'sent_probs': F.softmax(cos_sim,dim=-1),\n",
    "                       'phrase_probs': F.softmax(cos_phrase_sim,dim=-1)}\n",
    "        \n",
    "        return return_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d8637-1627-47c3-b87e-5c3cf1ed57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator, notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1fcd9-7fb7-4557-a1ee-023591930420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(discriminator,train_dl,valid_dl,epochs,batch_size,optimizer,PATIENCE=20,save_dir=None):\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    accelerator = Accelerator()\n",
    "    discriminator.device = accelerator.device\n",
    "    \n",
    "    discriminator, train_dl, valid_dl, optimizer = accelerator.prepare(discriminator, train_dl, valid_dl, optimizer)\n",
    "    \n",
    "\n",
    "    def validate(discriminator,valid_dl):\n",
    "    \n",
    "        discriminator.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for b in valid_dl:\n",
    "                y = discriminator(b)\n",
    "                              # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                              # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "                loss = y['sent_ce_loss'] + y['avg_phrase_ce_loss'] #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "                valid_loss += loss.item()\n",
    "                \n",
    "        accelerator.print(\"Validation Loss:\",valid_loss)\n",
    "        return valid_loss\n",
    "    \n",
    "    discriminator.train()\n",
    "    \n",
    "    loss_acc = 0\n",
    "    num_batches = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    patience = PATIENCE\n",
    "    \n",
    "    train_loss_arr,valid_loss_arr = [],[]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    discriminator.zero_grad()\n",
    "    \n",
    "    valid_loss = validate(discriminator,valid_dl)\n",
    "    valid_loss_arr.append(valid_loss/len(valid_dl))\n",
    "    best_valid_loss = valid_loss\n",
    "    \n",
    "    for E in range(epochs):\n",
    "        \n",
    "        num_samples = 0\n",
    "        \n",
    "        for b in train_dl:\n",
    "            \n",
    "            y = discriminator(b)\n",
    "                          # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                          # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "            loss = y['sent_ce_loss'] + y['avg_phrase_ce_loss'] #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "            \n",
    "            num_samples+=1\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            loss_acc += loss.item()\n",
    "            \n",
    "            if num_samples%batch_size==0:\n",
    "                optimizer.step()\n",
    "\n",
    "                num_batches += 1\n",
    "                total_steps += 1\n",
    "            \n",
    "                train_loss_arr.append(loss_acc/num_batches)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                if total_steps%100==0 and total_steps!=0:\n",
    "                    accelerator.print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "            \n",
    "        #print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "        \n",
    "        # torch.save({'model_state':discriminator.state_dict(),\n",
    "        #             'optimizer':optimizer.state_dict(),\n",
    "        #             'epoch':E},\n",
    "        #             f\"{save_dir}/Epoch_{E}_model_chkpt.pth.tar\")\n",
    "        \n",
    "        valid_loss = validate(discriminator,valid_dl)\n",
    "        valid_loss_arr.append(valid_loss/len(valid_dl))\n",
    "        \n",
    "        if valid_loss<best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience = PATIENCE\n",
    "            \n",
    "            accelerator.wait_for_everyone()\n",
    "            # if accelerator.is_main_process:\n",
    "            #     tokenizer.save_pretrained('Span_Llama_Checkpoints/')\n",
    "            # unwrapped_model = accelerator.unwrap_model(discriminator)\n",
    "            state_dict = accelerator.get_state_dict(discriminator)\n",
    "            torch.save({'model_dict':state_dict},f'{save_dir}/best_model_chkpt.pth.tar')\n",
    "        else:\n",
    "            patience -= 1\n",
    "            accelerator.print(f\"REDUCING PATIENCE...{patience}\")\n",
    "\n",
    "        if patience<=0:\n",
    "            accelerator.print(\"RUNNING OUT OF PATIENCE... TERMINATING\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return train_loss_arr,valid_loss_arr\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2da47-f00a-483d-b469-61dae4d8c2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# MPNet = AutoModel.from_pretrained(bert_chkpt).to(device)\n",
    "discriminator_model = discriminator(bert_chkpt)\n",
    "\n",
    "optimizer = torch.optim.AdamW(discriminator_model.parameters(),lr=1e-5)\n",
    "\n",
    "save_dir = 'Detect_Span_FB_Llama_chkpts_1'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e684a-f9c0-4225-8ccc-5bd36a5df048",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, valid_loss = notebook_launcher(train,args=(discriminator_model,train_DL,valid_DL,EPOCHS,BATCH_SIZE,optimizer,5,save_dir),num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe56f6-d721-4f15-a7eb-9ba96c619e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss,valid_loss = train(discriminator_model,\n",
    "                              train_DL,\n",
    "                              valid_DL,\n",
    "                              EPOCHS,\n",
    "                              BATCH_SIZE,\n",
    "                              optimizer,\n",
    "                              PATIENCE=5,\n",
    "                              save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91101fb6-ccc5-4b70-a781-f80306d79560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train_loss.json','w') as f:\n",
    "    json.dump(train_loss,f)\n",
    "\n",
    "with open('valid_loss.json','w') as f:\n",
    "    json.dump(valid_loss,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1438eb8-e8f1-41bd-ab9c-8c0ee15ab891",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ds = np.array(train_loss)[np.round(np.linspace(0, len(train_loss) - 1, len(valid_loss))).astype(int)]\n",
    "loss_df = pd.DataFrame({'train_loss':train_loss_ds , 'valid_loss':valid_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32161e3-1a96-4893-ac8a-5b567044334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as px\n",
    "px.line(loss_df,y=['train_loss','valid_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b9d00-0197-4bcc-be96-83e62d9c96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.load_state_dict(torch.load('GenFB_BART_chkpts_1/Epoch_0_model_chkpt.pth.tar')['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3a035-e5a3-49e4-bbf4-6a0699945e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for b in train_DL:\n",
    "    out = discriminator.generate(inputs=b['input'][0:1,0].to(device),top_p=0.5)\n",
    "    print(tokenizer.decode(b['input'][0:1,0][0],skip_special_tokens=True))\n",
    "    print(tokenizer.decode(b['feedback'][0:1,0][0],skip_special_tokens=True))\n",
    "    print(tokenizer.decode(out[0]))\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    i+=1\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96357d85-64e1-4d7e-8eab-ee36d405bbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
