{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccce523c-f7e2-459c-89d3-775573e476e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset feedback_qa (/home/raja/.cache/huggingface/datasets/McGill-NLP___feedback_qa/plain_text/1.0.0/20c8f938f417c88303bb7041cea9554c1d14667686d7d7c5dda83dd4f39e5dc4)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02359938621520996,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 43,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062ea3adf27441dc807a3b92fb8fe4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "\n",
    "from torch import nn\n",
    "from nltk import tokenize as nltk_tokenizer\n",
    "\n",
    "dataset = load_dataset(\"McGill-NLP/feedbackQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4626aded-95e1-47ec-bb47-59914940c8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rating_scores = {'Excellent':3 , 'Acceptable':2 , 'Could be Improved':1, 'Bad': -1}\n",
    "\n",
    "def process_df(df):\n",
    "    df['question'] = df['question'].apply(lambda x: x.replace('\\n',' '))\n",
    "    df['answer'] = df['answer'].apply(lambda x: x.replace('\\n',' '))\n",
    "    df['list_feedback'] = df['feedback'].apply(lambda x: [ r + \"___\" + e for r,e in zip(x['rating'],x['explanation']) ])\n",
    "    df['sampled_feedback'] = df['list_feedback'].apply(lambda x: np.random.choice(x).split(\"___\") )\n",
    "    df['rating_score'] = df['sampled_feedback'].apply(lambda x: rating_scores[x[0]])\n",
    "    df['rating'] = df['sampled_feedback'].apply(lambda x: x[0])\n",
    "    df['explanation'] = df['sampled_feedback'].apply(lambda x: x[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd2ffa3d-fae5-45d9-89c6-899dba3982fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = process_df(pd.DataFrame(dataset['train']))\n",
    "val_df = process_df(pd.DataFrame(dataset['validation']))\n",
    "test_df = process_df(pd.DataFrame(dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40dc46db-a864-459a-8e7a-b82df484643b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "bert_chkpt = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_chkpt)\n",
    "model = AutoModel.from_pretrained(bert_chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31c623b-2b7b-4b28-a499-b63ba8311ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '[UNK]', '<pad>', '<mask>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744c4863-8096-4c67-b473-90e03a0df39c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>feedback</th>\n",
       "      <th>list_feedback</th>\n",
       "      <th>sampled_feedback</th>\n",
       "      <th>rating_score</th>\n",
       "      <th>rating</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>Coronavirus (COVID-19) information for job see...</td>\n",
       "      <td>{'rating': ['Excellent', 'Could be Improved'],...</td>\n",
       "      <td>[Excellent___Has a link to detailed informatio...</td>\n",
       "      <td>[Excellent, Has a link to detailed information...</td>\n",
       "      <td>3</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Has a link to detailed information about gover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>Coronavirus (COVID-19) information for job see...</td>\n",
       "      <td>{'rating': ['Excellent', 'Excellent'], 'explan...</td>\n",
       "      <td>[Excellent___A link to a job search website is...</td>\n",
       "      <td>[Excellent, Includes a link to a Jobs Hub page...</td>\n",
       "      <td>3</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Includes a link to a Jobs Hub page, which is b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>Coronavirus (COVID-19) information and support...</td>\n",
       "      <td>{'rating': ['Bad', 'Acceptable'], 'explanation...</td>\n",
       "      <td>[Bad___Talks about tax credits for businesses ...</td>\n",
       "      <td>[Acceptable, This answer discusses the Employm...</td>\n",
       "      <td>2</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>This answer discusses the Employment Fund, whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If I am in Australia on a worker holiday marke...</td>\n",
       "      <td>Frequently Asked Questions Working holiday mak...</td>\n",
       "      <td>{'rating': ['Could be Improved', 'Acceptable']...</td>\n",
       "      <td>[Could be Improved___Answer is about Working H...</td>\n",
       "      <td>[Could be Improved, Answer is about Working Ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>Could be Improved</td>\n",
       "      <td>Answer is about Working Holiday Makers, but do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If I am in Australia on a worker holiday marke...</td>\n",
       "      <td>Frequently Asked Questions COVID-19 Pandemic -...</td>\n",
       "      <td>{'rating': ['Bad', 'Could be Improved'], 'expl...</td>\n",
       "      <td>[Bad___Discusses pandemic visas. Doesn't menti...</td>\n",
       "      <td>[Bad, Discusses pandemic visas. Doesn't mentio...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Discusses pandemic visas. Doesn't mention the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                   How do I get help finding a job?   \n",
       "1                   How do I get help finding a job?   \n",
       "2                   How do I get help finding a job?   \n",
       "3  If I am in Australia on a worker holiday marke...   \n",
       "4  If I am in Australia on a worker holiday marke...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Coronavirus (COVID-19) information for job see...   \n",
       "1  Coronavirus (COVID-19) information for job see...   \n",
       "2  Coronavirus (COVID-19) information and support...   \n",
       "3  Frequently Asked Questions Working holiday mak...   \n",
       "4  Frequently Asked Questions COVID-19 Pandemic -...   \n",
       "\n",
       "                                            feedback  \\\n",
       "0  {'rating': ['Excellent', 'Could be Improved'],...   \n",
       "1  {'rating': ['Excellent', 'Excellent'], 'explan...   \n",
       "2  {'rating': ['Bad', 'Acceptable'], 'explanation...   \n",
       "3  {'rating': ['Could be Improved', 'Acceptable']...   \n",
       "4  {'rating': ['Bad', 'Could be Improved'], 'expl...   \n",
       "\n",
       "                                       list_feedback  \\\n",
       "0  [Excellent___Has a link to detailed informatio...   \n",
       "1  [Excellent___A link to a job search website is...   \n",
       "2  [Bad___Talks about tax credits for businesses ...   \n",
       "3  [Could be Improved___Answer is about Working H...   \n",
       "4  [Bad___Discusses pandemic visas. Doesn't menti...   \n",
       "\n",
       "                                    sampled_feedback  rating_score  \\\n",
       "0  [Excellent, Has a link to detailed information...             3   \n",
       "1  [Excellent, Includes a link to a Jobs Hub page...             3   \n",
       "2  [Acceptable, This answer discusses the Employm...             2   \n",
       "3  [Could be Improved, Answer is about Working Ho...             1   \n",
       "4  [Bad, Discusses pandemic visas. Doesn't mentio...            -1   \n",
       "\n",
       "              rating                                        explanation  \n",
       "0          Excellent  Has a link to detailed information about gover...  \n",
       "1          Excellent  Includes a link to a Jobs Hub page, which is b...  \n",
       "2         Acceptable  This answer discusses the Employment Fund, whi...  \n",
       "3  Could be Improved  Answer is about Working Holiday Makers, but do...  \n",
       "4                Bad  Discusses pandemic visas. Doesn't mention the ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9078f4a5-10f0-4d44-886e-53ae3826ea30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coronavirus (COVID-19) information for job seekers Exisiting job seekers If you are a current job seeker or participant, this fact sheet provides important information about mutual obligation requirements, appointments with your provider, and what to do if you are self-isolating:  Information for job seekers and participants  If you are participating in the ParentsNext program, this fact sheet provides important information about your activities and appointments.   Information for ParentsNext participants   ParentsNext participants Frequently Asked Questions   If you are a New Business Assistance with NEIS participant, these Frequently Asked Questions (FAQ) provides information about accessing the Coronavirus Supplement and what support is available during this time:  New Business Assistance with NEIS participants - Frequently Asked Questions  If you are a New Business Assistance with NEIS provider, these Frequently Asked Questions (FAQ) provides information about supporting NEIS participants during the Coronavirus situation.  New Business Assistance with NEIS providers – Frequently Asked Questions  *[NEIS]: New Enterprise Incentive Scheme'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['answer'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619de540-d154-431a-8d5e-bab3bba6d7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  7596,  1014,  2133,  2028,  2021,  2729,  1033,     2, 19614,\n",
       "          4139,  9545,  9545,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'length': tensor([14])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Hello, how are you doing?'+ f\" {tokenizer.eos_token} \" + \"Hemlooooo\",add_special_tokens=True,return_tensors='pt', return_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af181ac2-b1ae-4fcd-b59d-74b4defb3d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_tokenizer.sent_tokenize(train_df['answer'].loc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f22dd1b8-d28f-4d16-9acc-cd3dce4579cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[21891, 23354, 1010, 2526, 17262, 1015, 2543, 1011, 2596, 2009, 3109, 24075, 4658, 17421, 3440, 3109, 24075, 2069, 2021, 2028, 1041, 2787, 3109, 29448, 2034, 13184, 1014, 2027, 2759, 7127, 3644, 2594, 2596, 2059, 8207, 14991, 5922, 1014, 14655, 2011, 2119, 10806, 1014, 2002, 2058, 2004, 2083, 2069, 2021, 2028, 2973, 1015, 11167, 22252, 1028, 2596, 2009, 3109, 24075, 2002, 6822, 2069, 2021, 2028, 8023, 2003, 2000, 3012, 2642, 18417, 2569, 1014, 2027, 2759, 7127, 3644, 2594, 2596, 2059, 2119, 3454, 2002, 14655, 1016], [2596, 2009, 3012, 2642, 18417, 6822, 3012, 2642, 18417, 6822, 4707, 2360, 3984, 2069, 2021, 2028, 1041, 2051, 2453, 5379, 2011, 11269, 2487, 13184, 1014, 2126, 4707, 2360, 3984, 1010, 6908, 4164, 1011, 3644, 2596, 2059, 3233, 2079, 2000, 21891, 23354, 12452, 2002, 2058, 2494, 2007, 2804, 2080, 2027, 2055, 1028, 2051, 2453, 5379, 2011, 11269, 2487, 6822, 1015, 4707, 2360, 3984, 2069, 2021, 2028, 1041, 2051, 2453, 5379, 2011, 11269, 2487, 10806, 1014, 2126, 4707, 2360, 3984, 1010, 6908, 4164, 1011, 3644, 2596, 2059, 4641, 11269, 2487, 6822, 2080, 2000, 21891, 23354, 3667, 1016], [2051, 2453, 5379, 2011, 11269, 2487, 11674, 1520, 4707, 2360, 3984, 1012, 1035, 11269, 2487, 1037, 1028, 2051, 6964, 20442, 5683]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_inp = tokenizer(nltk_tokenizer.sent_tokenize(train_df['answer'].loc[0]),add_special_tokens=False,return_token_type_ids=True)#,max_length=200,padding='max_length')\n",
    "tok_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e6d281-2513-459b-a854-3f099d26309b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Hello, how are you?',return_special_tokens_mask=True,add_special_tokens=True, padding='max_length', max_length=20)['special_tokens_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a093a991-7536-4082-9bd2-7602825edfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class feedback_QA_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,df,max_length=512):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.data = []\n",
    "        skipped = 0\n",
    "        \n",
    "        for i in tqdm.tqdm(range(len(self.df)),desc='vectorizing..'):\n",
    "            \n",
    "            d = {}\n",
    "            \n",
    "            tok_question = tokenizer('Question: ' + self.df.iloc[i]['question'] + ' Answer: ', add_special_tokens=False)\n",
    "            tok_answer = tokenizer(self.df.iloc[i]['answer'], add_special_tokens=False)\n",
    "            tok_feedback = tokenizer(self.df.iloc[i]['explanation'], add_special_tokens=False)\n",
    "            \n",
    "            if len(tok_question['input_ids']+tok_answer['input_ids']+tok_feedback['input_ids'])+4 > self.max_len:\n",
    "                skipped +=1\n",
    "                continue\n",
    "            \n",
    "            context = [tokenizer.bos_token_id] + tok_question['input_ids'] + tok_answer['input_ids']\n",
    "            context_attn = [1] + tok_question['attention_mask'] + tok_answer['attention_mask']\n",
    "            context_pool_mask = [0] + [0]*len(tok_question['input_ids']) + tok_answer['attention_mask']\n",
    "            \n",
    "            \n",
    "            d['context_w_feedback'] = context + [tokenizer.sep_token_id]*2 + tok_feedback['input_ids'] + [tokenizer.eos_token_id]\n",
    "            \n",
    "            PAD_LEN = self.max_len - len(d['context_w_feedback'])\n",
    "            \n",
    "            d['context_w_feedback'] += [tokenizer.pad_token_id]*PAD_LEN\n",
    "            d['context_w_feedback_attn'] = context_attn + [1,1] + tok_feedback['attention_mask'] + [1] + [0]*PAD_LEN            \n",
    "            d['context'] = d['context_w_feedback']\n",
    "            d['context_attn'] = context_attn + [1,0] + [0]*len(tok_feedback['attention_mask']) + [0] + [0]*PAD_LEN\n",
    "            \n",
    "            d['feedback_pool_mask'] = [0]*len(context_pool_mask) + [0,0] + tok_feedback['attention_mask'] + [0] + [0]*PAD_LEN\n",
    "            d['answer_pool_mask'] = context_pool_mask + [0,0] + [0]*len(tok_feedback['attention_mask']) + [0] + [0]*PAD_LEN\n",
    "            \n",
    "            answer_phrases = nltk_tokenizer.sent_tokenize(self.df.iloc[i]['answer'])\n",
    "            tok_phrases = tokenizer(answer_phrases,add_special_tokens=False,return_token_type_ids=True)\n",
    "            \n",
    "            d['answer_phrases_pool_mask'] = []\n",
    "            \n",
    "            for j in range(len(answer_phrases)):\n",
    "                answer_phrases_attn_mask = tok_phrases['token_type_ids'].copy()\n",
    "                answer_phrases_attn_mask[j] = tok_phrases['attention_mask'][j].copy()\n",
    "                answer_phrases_attn_mask = list(itertools.chain.from_iterable(answer_phrases_attn_mask))\n",
    "                # pad_len = len(tok_answer['attention_mask']) - len(answer_phrases_attn_mask)\n",
    "                # answer_phrases_attn_mask += [0]*pad_len\n",
    "                \n",
    "                answer_phrase_pool_mask = [0] + [0]*len(tok_question['input_ids']) + answer_phrases_attn_mask + [0,0] + [0]*len(tok_feedback['attention_mask']) + [0] + [0]*PAD_LEN\n",
    "                \n",
    "                d['answer_phrases_pool_mask'].append(answer_phrase_pool_mask)\n",
    "            \n",
    "            if len(d['answer_phrases_pool_mask'][0])>len(d['answer_pool_mask']):\n",
    "                skipped +=1\n",
    "                continue\n",
    "                \n",
    "            self.data.append(d)\n",
    "        print('Skipped: ',skipped)\n",
    "\n",
    "    def add_neg_samples(self):\n",
    "        for i in tqdm.tqdm(range(self.__len__()),desc='adding neg samples...'):\n",
    "            self.data[i]['feedback_set'] = [self.data[i]['context_w_feedback']]\n",
    "            self.data[i]['feedback_attn_set'] = [self.data[i]['context_w_feedback_attn']]\n",
    "            self.data[i]['feedback_pool_mask_set'] = [self.data[i]['feedback_pool_mask']]\n",
    "            L = list(range(self.__len__()))\n",
    "            L.remove(i)\n",
    "            neg_samples_idx = np.random.choice(L,size=4)\n",
    "            for n_id in neg_samples_idx:\n",
    "                self.data[i]['feedback_set'].append(self.data[n_id]['context_w_feedback'])\n",
    "                self.data[i]['feedback_attn_set'].append(self.data[n_id]['context_w_feedback_attn'])\n",
    "                self.data[i]['feedback_pool_mask_set'].append(self.data[n_id]['feedback_pool_mask'])\n",
    "            for k in self.data[i].keys():\n",
    "                self.data[i][k] = torch.tensor(self.data[i][k])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c69c540d-1963-419c-8c09-08f5d65aa803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vectorizing..:   0%|                                                                                                                           | 0/1995 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1323 > 512). Running this sequence through the model will result in indexing errors\n",
      "vectorizing..: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1995/1995 [00:06<00:00, 325.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped:  173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding neg samples...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1822/1822 [00:08<00:00, 224.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = feedback_QA_dataset(train_df)\n",
    "# train_dataset.add_neg_samples()\n",
    "# valid_dataset = feedback_QA_dataset(val_df)\n",
    "# valid_dataset.add_neg_samples()\n",
    "test_dataset = feedback_QA_dataset(test_df)\n",
    "test_dataset.add_neg_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92049336-0c27-4e1d-b5f7-9791745c5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_DL = DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "# valid_DL = DataLoader(valid_dataset,batch_size=1,shuffle=True)\n",
    "test_DL = DataLoader(test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1da63016-24f0-400e-9a29-9093ab832bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_w_feedback torch.Size([1, 512])\n",
      "context_w_feedback_attn torch.Size([1, 512])\n",
      "context torch.Size([1, 512])\n",
      "context_attn torch.Size([1, 512])\n",
      "feedback_pool_mask torch.Size([1, 512])\n",
      "answer_pool_mask torch.Size([1, 512])\n",
      "answer_phrases_pool_mask torch.Size([1, 2, 512])\n",
      "feedback_set torch.Size([1, 5, 512])\n",
      "feedback_attn_set torch.Size([1, 5, 512])\n",
      "feedback_pool_mask_set torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "for b in test_DL:\n",
    "    for k in b.keys():\n",
    "        print(k,b[k].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0015532b-ac2e-4a8b-b7a0-00bd31a5289f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = AutoModel.from_pretrained(bert_chkpt).to(device)\n",
    "# model.load_state_dict(torch.load('Detect_Span_FB_MPNET_chkpts_1/best_model_chkpt.pth.tar')['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "120c4acd-da1e-4c40-a422-a3c3161314b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chkpt = torch.load('Detect_Span_FB_MPNET_chkpts_3/Epoch_0_model_chkpt.pth.tar')\n",
    "from collections import OrderedDict\n",
    "model_state = OrderedDict([(k[6:],v) for k,v in chkpt['model_state'].items()])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac79ab78-46b5-4711-9f1f-50b963f2eee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "524eef71-2aad-4803-892f-01b0a6fa82de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:  question : what are my options if i can not support myself on a whm visa? answer : frequently asked questions covid - 19 pandemic - australian government endorsed event ( agee ) stream of the temporary activity ( subclass 408 ) visa frequently asked questions when can i apply for the covid - 19 pandemic event visa? you should only apply for this visa is you are unable to depart australia, your temporary visa expires in less than 28 days ( or did not expire more than 28 days ago ) and you have no other visa options available to you. \n",
      "\n",
      "Feedback:  this is off task, the answer is more about an expired visa \n",
      "\n",
      "Phrase 0: frequently asked questions covid - 19 pandemic - australian government endorsed event ( agee ) stream of the temporary activity ( subclass 408 ) visa frequently asked questions when can i apply for the covid - 19 pandemic event visa?\n",
      "Relevance of phrase 0 is -0.0016022920608520508 \n",
      "\n",
      "Phrase 1: you should only apply for this visa is you are unable to depart australia, your temporary visa expires in less than 28 days ( or did not expire more than 28 days ago ) and you have no other visa options available to you.\n",
      "Relevance of phrase 1 is 0.0012790709733963013 \n",
      "\n",
      "----------------------------\n",
      "\n",
      "Input:  question : i was a working holiday maker but lost my job ; what should i do? answer : frequently asked questions covid - 19 pandemic - australian government endorsed event ( agee ) stream of the temporary activity ( subclass 408 ) visa frequently asked questions when can i apply for the covid - 19 pandemic event visa? you should only apply for this visa is you are unable to depart australia, your temporary visa expires in less than 28 days ( or did not expire more than 28 days ago ) and you have no other visa options available to you. \n",
      "\n",
      "Feedback:  this talks about visa expiration and does not answer our question on what should be done. \n",
      "\n",
      "Phrase 0: frequently asked questions covid - 19 pandemic - australian government endorsed event ( agee ) stream of the temporary activity ( subclass 408 ) visa frequently asked questions when can i apply for the covid - 19 pandemic event visa?\n",
      "Relevance of phrase 0 is -0.013561517000198364 \n",
      "\n",
      "Phrase 1: you should only apply for this visa is you are unable to depart australia, your temporary visa expires in less than 28 days ( or did not expire more than 28 days ago ) and you have no other visa options available to you.\n",
      "Relevance of phrase 1 is 0.005836963653564453 \n",
      "\n",
      "----------------------------\n",
      "\n",
      "Input:  question : is it practical to expect children to practice social distancing in childcare settings? answer : physical distancing for coronavirus ( covid - 19 ) in schools if your child is sick, they must not go to school or childcare. you must keep them at home and away from others. to reduce the spread of viruses or germs in schools students and staff should continue to practise good hygiene. the australian health protection principal committee ( ahppc ) has issued updated advice on reducing the potential risk of covid - 19 transmission in schools. the ahppc also issued a statement on risk management for re - opening boarding schools and school - based residential colleges. for more information on school operations, visit the department of education, skills and employment website. \n",
      "\n",
      "Feedback:  this answer touches on children and the virus but doesn't really answer the question. there is nothing stating whether children should be expected to follow all the protocals. \n",
      "\n",
      "Phrase 0: physical distancing for coronavirus ( covid - 19 ) in schools if your child is sick, they must not go to school or childcare.\n",
      "Relevance of phrase 0 is -0.0018391013145446777 \n",
      "\n",
      "Phrase 1: you must keep them at home and away from others.\n",
      "Relevance of phrase 1 is 0.0028623640537261963 \n",
      "\n",
      "Phrase 2: to reduce the spread of viruses or germs in schools students and staff should continue to practise good hygiene.\n",
      "Relevance of phrase 2 is 0.0010966360569000244 \n",
      "\n",
      "Phrase 3: the australian health protection principal committee ( ahppc ) has issued updated advice on reducing the potential risk of covid - 19 transmission in schools.\n",
      "Relevance of phrase 3 is -0.011521518230438232 \n",
      "\n",
      "Phrase 4: the ahppc also issued a statement on risk management for re - opening boarding schools and school - based residential colleges.\n",
      "Relevance of phrase 4 is -0.008349895477294922 \n",
      "\n",
      "Phrase 5: for more information on school operations, visit the department of education, skills and employment website.\n",
      "Relevance of phrase 5 is -0.007508754730224609 \n",
      "\n",
      "----------------------------\n",
      "\n",
      "Input:  question : do i have to continue making gym membership payments if my gym has closed due to the coronavirus outbreak? answer : covid - 19 ( coronavirus ) information for consumers gym memberships can my gym charge me a membership ‘ freeze ’ or ‘ holding ’ fee for the period they are closed? membership ‘ freeze ’ or ‘ holding ’ fees may be charged by gyms when customers elect to pause their membership, if this is permitted by the terms and conditions. given many memberships are being paused due to the government restrictions preventing gyms from operating, rather than customers requesting a pause, the accc expects that gyms will not charge membership ‘ freeze ’ or ‘ holding ’ fees. the accc also expects that gyms will refund any such holding fees incorrectly charged since the government restrictions came into effect. \n",
      "\n",
      "Feedback:  this is a good answer. it gives examples of what a gym might do with fees if they are forced to close. \n",
      "\n",
      "Phrase 0: covid - 19 ( coronavirus ) information for consumers gym memberships can my gym charge me a membership ‘ freeze ’ or ‘ holding ’ fee for the period they are closed?\n",
      "Relevance of phrase 0 is -0.002281695604324341 \n",
      "\n",
      "Phrase 1: membership ‘ freeze ’ or ‘ holding ’ fees may be charged by gyms when customers elect to pause their membership, if this is permitted by the terms and conditions.\n",
      "Relevance of phrase 1 is 0.0017688870429992676 \n",
      "\n",
      "Phrase 2: given many memberships are being paused due to the government restrictions preventing gyms from operating, rather than customers requesting a pause, the accc expects that gyms will not charge membership ‘ freeze ’ or ‘ holding ’ fees.\n",
      "Relevance of phrase 2 is -0.0017186999320983887 \n",
      "\n",
      "Phrase 3: the accc also expects that gyms will refund any such holding fees incorrectly charged since the government restrictions came into effect.\n",
      "Relevance of phrase 3 is -0.006104260683059692 \n",
      "\n",
      "----------------------------\n",
      "\n",
      "Input:  question : can i keep on working while waiting to see if the application for my next whm visa is accepted or not? answer : frequently asked questions temporary visa measures supporting the agriculture sector employers due to the covid - 19 border restrictions, seasonal workers and pacific labour scheme participants are unable to travel to australia at present. what other options are available to access workers to address labour needs? in response to the current covid - 19 pandemic, the australian government has announced temporary measures to assist temporary visa holders currently in australia, including seasonal worker programme and pacific labour scheme participants, who are unable to currently return to their home country, to extend their stay in australia, and enable flexibility in changing approved employers where required. under these temporary measures, current approved employers may wish to employ seasonal worker programme or pacific labour scheme participants who have finished employment with their current approved employer, but who are unable to return to their home country. approved employers, like any employer, may also wish to employ temporary activity ( subclass 408 ) australian government endorsed event ( agee ) stream visa holders this visa will have a nil visa application charge ( vac ) for the covid - 19 pandemic event. employer sponsorship arrangements similar to the seasonal worker programme will also apply to the subclass 408 visa. these temporary measures are not intended to prevent the recruitment of australians to undertake this work. before seeking access to seasonal workers under the seasonal worker programme, approved employers must first try to recruit australians. where australian workers are unavailable, employers can also seek seasonal labour through the working holiday maker program. working holiday makers who are working in critical sectors ( i. e. agriculture, food processing, health care, aged care, disability care or child care ) will be exempt from the six month work limitation with one employer and eligible for a temporary activity ( subclass 408 ) visa in the australian government endorsed event ( agee ) stream. employers are still required to abide by all relevant australian workplace laws. overseas workers have the same rights under australian workplace law as all other employees. these temporary measures will be in place for a timeframe that allows relevant critical industries to bridge the gap between their immediate needs and the time to recruit, train and on - board australians. the department of home affairs is working with the department of education, skills and employment to ensure australians are prioritised for future job opportunities. \n",
      "\n",
      "Feedback:  this answers the questions to some certain extent, but the answer can still be improved. \n",
      "\n",
      "Phrase 0: frequently asked questions temporary visa measures supporting the agriculture sector employers due to the covid - 19 border restrictions, seasonal workers and pacific labour scheme participants are unable to travel to australia at present.\n",
      "Relevance of phrase 0 is -4.026293754577637e-05 \n",
      "\n",
      "Phrase 1: what other options are available to access workers to address labour needs?\n",
      "Relevance of phrase 1 is 0.0023541152477264404 \n",
      "\n",
      "Phrase 2: in response to the current covid - 19 pandemic, the australian government has announced temporary measures to assist temporary visa holders currently in australia, including seasonal worker programme and pacific labour scheme participants, who are unable to currently return to their home country, to extend their stay in australia, and enable flexibility in changing approved employers where required.\n",
      "Relevance of phrase 2 is -0.0048309266567230225 \n",
      "\n",
      "Phrase 3: under these temporary measures, current approved employers may wish to employ seasonal worker programme or pacific labour scheme participants who have finished employment with their current approved employer, but who are unable to return to their home country.\n",
      "Relevance of phrase 3 is 0.0003050118684768677 \n",
      "\n",
      "Phrase 4: approved employers, like any employer, may also wish to employ temporary activity ( subclass 408 ) australian government endorsed event ( agee ) stream visa holders this visa will have a nil visa application charge ( vac ) for the covid - 19 pandemic event.\n",
      "Relevance of phrase 4 is -0.005203753709793091 \n",
      "\n",
      "Phrase 5: employer sponsorship arrangements similar to the seasonal worker programme will also apply to the subclass 408 visa.\n",
      "Relevance of phrase 5 is -0.001127392053604126 \n",
      "\n",
      "Phrase 6: these temporary measures are not intended to prevent the recruitment of australians to undertake this work.\n",
      "Relevance of phrase 6 is 0.0014612674713134766 \n",
      "\n",
      "Phrase 7: before seeking access to seasonal workers under the seasonal worker programme, approved employers must first try to recruit australians.\n",
      "Relevance of phrase 7 is 0.0021024495363235474 \n",
      "\n",
      "Phrase 8: where australian workers are unavailable, employers can also seek seasonal labour through the working holiday maker program.\n",
      "Relevance of phrase 8 is 0.0019436478614807129 \n",
      "\n",
      "Phrase 9: working holiday makers who are working in critical sectors ( i. e.\n",
      "Relevance of phrase 9 is 0.0005118399858474731 \n",
      "\n",
      "Phrase 10: agriculture, food processing, health care, aged care, disability care or child care ) will be exempt from the six month work limitation with one employer and eligible for a temporary activity ( subclass 408 ) visa in the australian government endorsed event ( agee ) stream.\n",
      "Relevance of phrase 10 is 9.912252426147461e-05 \n",
      "\n",
      "Phrase 11: employers are still required to abide by all relevant australian workplace laws.\n",
      "Relevance of phrase 11 is -0.0023084431886672974 \n",
      "\n",
      "Phrase 12: overseas workers have the same rights under australian workplace law as all other employees.\n",
      "Relevance of phrase 12 is -0.005908355116844177 \n",
      "\n",
      "Phrase 13: these temporary measures will be in place for a timeframe that allows relevant critical industries to bridge the gap between their immediate needs and the time to recruit, train and on - board australians.\n",
      "Relevance of phrase 13 is 0.001738712191581726 \n",
      "\n",
      "Phrase 14: the department of home affairs is working with the department of education, skills and employment to ensure australians are prioritised for future job opportunities.\n",
      "Relevance of phrase 14 is 0.0031019896268844604 \n",
      "\n",
      "----------------------------\n",
      "\n",
      "Input:  question : can i continue to work while i am waiting for a decision for my application for my whm visa? answer : frequently asked questions covid - 19 pandemic - australian government endorsed event ( agee ) stream of the temporary activity ( subclass 408 ) visa frequently asked questions i am overseas. can i be granted a covid - 19 pandemic event visa? the covid - 19 pandemic event visa can only be granted to people in australia. \n",
      "\n",
      "Feedback:  it does not state if the person can continue to work while waiting for the visa \n",
      "\n",
      "Phrase 0: frequently asked questions covid - 19 pandemic - australian government endorsed event ( agee ) stream of the temporary activity ( subclass 408 ) visa frequently asked questions i am overseas.\n",
      "Relevance of phrase 0 is -0.00266420841217041 \n",
      "\n",
      "Phrase 1: can i be granted a covid - 19 pandemic event visa?\n",
      "Relevance of phrase 1 is -0.010642021894454956 \n",
      "\n",
      "Phrase 2: the covid - 19 pandemic event visa can only be granted to people in australia.\n",
      "Relevance of phrase 2 is 0.0020273029804229736 \n",
      "\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    se = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return F.normalize(se, p=2, dim=1)\n",
    "\n",
    "j = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for b in test_DL:\n",
    "        se = mean_pooling( model(input_ids = b['context_w_feedback'].to(device),attention_mask=b['context_attn'].to(device))[0], b['answer_pool_mask'].to(device))\n",
    "        \n",
    "        fmo = model(input_ids = b['feedback_set'][0].to(device),attention_mask=b['feedback_attn_set'][0].to(device))[0]\n",
    "        # print(b['feedback_set'][0].shape, b['feedback_attn_set'][0].shape, fmo.shape, b['feedback_pool_mask_set'][0].shape)\n",
    "        \n",
    "        fe = mean_pooling(model(input_ids = b['feedback_set'][0].to(device),attention_mask=b['feedback_attn_set'][0].to(device))[0], b['feedback_pool_mask_set'][0].to(device))\n",
    "        \n",
    "        pmo = model(input_ids = b['context_w_feedback'].to(device),attention_mask=b['context_attn'].to(device))\n",
    "        # print(pmo[0].shape,b['answer_phrases_pool_mask'].shape,pmo[0].repeat(b['answer_phrases_pool_mask'][0].shape[0],1,1).shape,b['answer_phrases_pool_mask'][0].shape)\n",
    "        pe = mean_pooling(pmo[0].repeat(b['answer_phrases_pool_mask'][0].shape[0],1,1),b['answer_phrases_pool_mask'][0].to(device) )# for i in range(b['answer_phrases_pool_mask'][0].shape[0])]\n",
    "        # pe = torch.stack(pe).squeeze(1)\n",
    "        cos_sim = F.cosine_similarity(se,fe,dim=1)\n",
    "        cos_phrase_sim = torch.matmul(pe,fe.transpose(1,0))\n",
    "        # print(fe.shape,se.shape,pe.shape,cos_sim,cos_phrase_sim.mean(0))\n",
    "        \n",
    "        sent_probs = F.softmax(cos_sim,dim=-1)\n",
    "        phrase_probs = F.softmax(cos_phrase_sim,dim=-1)\n",
    "        \n",
    "        print('\\nInput: ',tokenizer.decode(torch.mul(b['context_w_feedback'][0],b['context_attn'][0]),skip_special_tokens=True),'\\n')\n",
    "        print('Feedback: ',tokenizer.decode(torch.mul(b['context_w_feedback'][0],b['feedback_pool_mask'][0]),skip_special_tokens=True),'\\n')\n",
    "        for i in range(b['answer_phrases_pool_mask'][0].shape[0]):\n",
    "            relevance = phrase_probs[i][0] - sent_probs[0]\n",
    "            \n",
    "            phrase_tok = torch.mul(b['context_w_feedback'][0],b['answer_phrases_pool_mask'][0][i])\n",
    "            print(f\"Phrase {i}:\",tokenizer.decode(phrase_tok,skip_special_tokens=True))\n",
    "            print(f\"Relevance of phrase {i} is {relevance}\",'\\n')\n",
    "        \n",
    "#         print('softmax: ',F.softmax(cos_sim),F.softmax(cos_phrase_sim,dim=-1))\n",
    "        \n",
    "#         tgt_tensor = torch.zeros(b['feedback_set'].shape[1] , device=device)\n",
    "#         tgt_tensor[0] = 1.0\n",
    "#         print('CE Loss: ', F.cross_entropy(cos_sim,target=tgt_tensor), F.cross_entropy(cos_phrase_sim.mean(0),target=torch.tensor([1.0,0,0,0,0]).to(device)))\n",
    "        print('----------------------------')\n",
    "        j+=1\n",
    "        if j>5:\n",
    "            break\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea145fc-02e2-4ca9-b042-540379617b56",
   "metadata": {},
   "source": [
    "## Evaluating over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9b830ac6-f34a-43d0-9658-2ffbd1cbc1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1822/1822 [03:43<00:00,  8.16it/s]\n"
     ]
    }
   ],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    se = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return F.normalize(se, p=2, dim=1)\n",
    "\n",
    "j = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    with open('baseline_test_preds.txt','w') as f:\n",
    "        for b in tqdm.tqdm(test_DL,desc='evaluating...'):\n",
    "            se = mean_pooling( model(input_ids = b['context_w_feedback'].to(device),attention_mask=b['context_attn'].to(device))[0], b['answer_pool_mask'].to(device))\n",
    "\n",
    "            fmo = model(input_ids = b['feedback_set'][0].to(device),attention_mask=b['feedback_attn_set'][0].to(device))[0]\n",
    "            # print(b['feedback_set'][0].shape, b['feedback_attn_set'][0].shape, fmo.shape, b['feedback_pool_mask_set'][0].shape)\n",
    "\n",
    "            fe = mean_pooling(model(input_ids = b['feedback_set'][0].to(device),attention_mask=b['feedback_attn_set'][0].to(device))[0], b['feedback_pool_mask_set'][0].to(device))\n",
    "\n",
    "            pmo = model(input_ids = b['context_w_feedback'].to(device),attention_mask=b['context_attn'].to(device))\n",
    "            # print(pmo[0].shape,b['answer_phrases_pool_mask'].shape,pmo[0].repeat(b['answer_phrases_pool_mask'][0].shape[0],1,1).shape,b['answer_phrases_pool_mask'][0].shape)\n",
    "            pe = mean_pooling(pmo[0].repeat(b['answer_phrases_pool_mask'][0].shape[0],1,1),b['answer_phrases_pool_mask'][0].to(device) )# for i in range(b['answer_phrases_pool_mask'][0].shape[0])]\n",
    "            # pe = torch.stack(pe).squeeze(1)\n",
    "            cos_sim = F.cosine_similarity(se,fe,dim=1)\n",
    "            cos_phrase_sim = torch.matmul(pe,fe.transpose(1,0))\n",
    "            # print(fe.shape,se.shape,pe.shape,cos_sim,cos_phrase_sim.mean(0))\n",
    "\n",
    "            sent_probs = F.softmax(cos_sim,dim=-1)\n",
    "            phrase_probs = F.softmax(cos_phrase_sim,dim=-1)\n",
    "\n",
    "            f.write(f\"\\nInput: {tokenizer.decode(torch.mul(b['context_w_feedback'][0],b['context_attn'][0]),skip_special_tokens=True)}\\n\\n\")\n",
    "            f.write(f\"Feedback: {tokenizer.decode(torch.mul(b['context_w_feedback'][0],b['feedback_pool_mask'][0]),skip_special_tokens=True)}\\n\\n\")\n",
    "            for i in range(b['answer_phrases_pool_mask'][0].shape[0]):\n",
    "                relevance = phrase_probs[i][0] - sent_probs[0]\n",
    "\n",
    "                phrase_tok = torch.mul(b['context_w_feedback'][0],b['answer_phrases_pool_mask'][0][i])\n",
    "                f.write(f\"Phrase {i}: {tokenizer.decode(phrase_tok,skip_special_tokens=True)}\\n\")\n",
    "                f.write(f\"Relevance of phrase {i} is {relevance}\\n\\n\")\n",
    "\n",
    "    #         print('softmax: ',F.softmax(cos_sim),F.softmax(cos_phrase_sim,dim=-1))\n",
    "\n",
    "    #         tgt_tensor = torch.zeros(b['feedback_set'].shape[1] , device=device)\n",
    "    #         tgt_tensor[0] = 1.0\n",
    "    #         print('CE Loss: ', F.cross_entropy(cos_sim,target=tgt_tensor), F.cross_entropy(cos_phrase_sim.mean(0),target=torch.tensor([1.0,0,0,0,0]).to(device)))\n",
    "            f.write('----------------------------')\n",
    "            # j+=1\n",
    "            # if j>5:\n",
    "            #     break\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0f3ed8bc-1d6e-47fa-bfbb-0f6b3b1d97b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4, 5],\n",
       "         [6, 7, 8, 9, 0]],\n",
       "\n",
       "        [[1, 2, 3, 4, 5],\n",
       "         [6, 7, 8, 9, 0]]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[[1,2,3,4,5],[6,7,8,9,0]]])\n",
    "t.repeat(2,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "39373ab0-572f-42ec-8bee-d6945dd568bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    def __init__(self, model_chkpt, device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = AutoModel.from_pretrained(model_chkpt).to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def mean_pooling(self,model_output,attention_mask):\n",
    "        token_embeddings = model_output #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        se = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return F.normalize(se, p=2, dim=1)\n",
    "        \n",
    "    def forward(self, b):\n",
    "        sent_model_out = self.model(input_ids = b['context_w_feedback'].to(self.device),attention_mask=b['context_attn'].to(self.device))[0]\n",
    "        feedback_model_out = self.model(input_ids = b['feedback_set'][0].to(self.device),attention_mask=b['feedback_attn_set'][0].to(self.device))[0]\n",
    "        \n",
    "        sent_emb = self.mean_pooling( sent_model_out, b['answer_pool_mask'].to(self.device))\n",
    "        feedback_emb = self.mean_pooling( feedback_model_out, b['feedback_pool_mask_set'][0].to(self.device))\n",
    "        \n",
    "        # print(pmo[0].shape,b['answer_phrases_pool_mask'].shape)\n",
    "        phrase_emb = self.mean_pooling( sent_model_out.repeat(b['answer_phrases_pool_mask'][0].shape[0],1,1), b['answer_phrases_pool_mask'][0].to(self.device) )\n",
    "        # phrase_emb = torch.stack(phrase_emb).squeeze(1)\n",
    "        cos_sim = F.cosine_similarity(sent_emb,feedback_emb,dim=1)\n",
    "        cos_phrase_sim = torch.matmul(phrase_emb,feedback_emb.transpose(1,0))\n",
    "        \n",
    "        tgt_tensor = torch.zeros(b['feedback_set'].shape[1] , device=self.device)\n",
    "        tgt_tensor[0] = 1.0 #the relevant feedback is always present at index 0\n",
    "        \n",
    "        return_dict = {'sent_ce_loss': F.cross_entropy(cos_sim,target=tgt_tensor),\n",
    "                       'avg_phrase_ce_loss': F.cross_entropy(cos_phrase_sim.mean(0),target=tgt_tensor),\n",
    "                       'sent_probs': F.softmax(cos_sim,dim=-1),\n",
    "                       'phrase_probs': F.softmax(cos_phrase_sim,dim=-1)}\n",
    "        \n",
    "        return return_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "97a1fcd9-7fb7-4557-a1ee-023591930420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(discriminator,train_dl,valid_dl,epochs,batch_size,optimizer,PATIENCE=20,save_dir=None):\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    \n",
    "    discriminator.train()\n",
    "    \n",
    "    loss_acc = 0\n",
    "    num_batches = 0\n",
    "    total_steps = 0\n",
    "    best_valid_loss = np.inf\n",
    "    patience = PATIENCE\n",
    "    \n",
    "    train_loss_arr,valid_loss_arr = [],[]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    discriminator.zero_grad()\n",
    "    \n",
    "    for E in range(epochs):\n",
    "        \n",
    "        valid_loss = validate(discriminator,valid_dl)\n",
    "        valid_loss_arr.append(valid_loss/len(valid_dl))\n",
    "        \n",
    "        num_samples = 0\n",
    "        \n",
    "        for b in train_dl:\n",
    "            \n",
    "            y = discriminator(b)\n",
    "                          # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                          # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "            loss = y['sent_ce_loss'] + y['avg_phrase_ce_loss'] #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "            \n",
    "            num_samples+=1\n",
    "            \n",
    "            loss.backward()\n",
    "            loss_acc += loss.item()\n",
    "            \n",
    "            if num_samples%batch_size==0:\n",
    "                optimizer.step()\n",
    "\n",
    "                num_batches += 1\n",
    "                total_steps += 1\n",
    "            \n",
    "                train_loss_arr.append(loss_acc/num_batches)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                if total_steps%100==0 and total_steps!=0:\n",
    "                    print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "            \n",
    "        #print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "        \n",
    "        torch.save({'model_state':discriminator.state_dict(),\n",
    "                    'optimizer':optimizer.state_dict(),\n",
    "                    'epoch':E},\n",
    "                    f\"{save_dir}/Epoch_{E}_model_chkpt.pth.tar\")\n",
    "        \n",
    "        if valid_loss<best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience = PATIENCE\n",
    "            \n",
    "            torch.save({'model_state':discriminator.state_dict(),\n",
    "                        'optimizer':optimizer.state_dict(),\n",
    "                        'epoch':E},\n",
    "                        f\"{save_dir}/best_model_chkpt.pth.tar\")\n",
    "        else:\n",
    "            patience -= 1\n",
    "            print(f\"REDUCING PATIENCE...{patience}\")\n",
    "\n",
    "        if patience<=0:\n",
    "            print(\"RUNNING OUT OF PATIENCE... TERMINATING\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return train_loss_arr,valid_loss_arr\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b5e5931d-65fc-421d-8bf3-4bdb58b8b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(discriminator,valid_dl):\n",
    "    \n",
    "    discriminator.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for b in valid_dl:\n",
    "            y = discriminator(b)\n",
    "                          # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                          # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "            loss = y['sent_ce_loss'] + y['avg_phrase_ce_loss'] #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    print(\"Validation Loss:\",valid_loss)\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1cb84b-dd2e-495f-845d-03ec726b7bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3223.12977540493\n",
      "Epoch: 0 \t Steps taken: 100 \tLoss: 29.962738287448882\n",
      "Epoch: 0 \t Steps taken: 200 \tLoss: 29.572392408251762\n",
      "Epoch: 0 \t Steps taken: 300 \tLoss: 29.432847184737522\n",
      "Validation Loss: 2379.9055646657944\n",
      "Epoch: 1 \t Steps taken: 400 \tLoss: 29.339444583654405\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "# MPNet = AutoModel.from_pretrained(bert_chkpt).to(device)\n",
    "discriminator_model = discriminator(bert_chkpt,device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(discriminator_model.parameters(),lr=1e-5)\n",
    "\n",
    "save_dir = 'Detect_Span_FB_MPNET_chkpts_3'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "train_loss,valid_loss = train(discriminator_model,\n",
    "                              train_DL,\n",
    "                              valid_DL,\n",
    "                              EPOCHS,\n",
    "                              BATCH_SIZE,\n",
    "                              optimizer,\n",
    "                              PATIENCE=5,\n",
    "                              save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91101fb6-ccc5-4b70-a781-f80306d79560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train_loss.json','w') as f:\n",
    "    json.dump(train_loss,f)\n",
    "\n",
    "with open('valid_loss.json','w') as f:\n",
    "    json.dump(valid_loss,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1438eb8-e8f1-41bd-ab9c-8c0ee15ab891",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ds = np.array(train_loss)[np.round(np.linspace(0, len(train_loss) - 1, len(valid_loss))).astype(int)]\n",
    "loss_df = pd.DataFrame({'train_loss':train_loss_ds , 'valid_loss':valid_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32161e3-1a96-4893-ac8a-5b567044334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as px\n",
    "px.line(loss_df,y=['train_loss','valid_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b9d00-0197-4bcc-be96-83e62d9c96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.load_state_dict(torch.load('GenFB_BART_chkpts_1/Epoch_0_model_chkpt.pth.tar')['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3a035-e5a3-49e4-bbf4-6a0699945e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for b in train_DL:\n",
    "    out = discriminator.generate(inputs=b['input'][0:1,0].to(device),top_p=0.5)\n",
    "    print(tokenizer.decode(b['input'][0:1,0][0],skip_special_tokens=True))\n",
    "    print(tokenizer.decode(b['feedback'][0:1,0][0],skip_special_tokens=True))\n",
    "    print(tokenizer.decode(out[0]))\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    i+=1\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96357d85-64e1-4d7e-8eab-ee36d405bbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
