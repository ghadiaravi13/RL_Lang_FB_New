{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccce523c-f7e2-459c-89d3-775573e476e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset feedback_qa (/home/raja/.cache/huggingface/datasets/McGill-NLP___feedback_qa/plain_text/1.0.0/20c8f938f417c88303bb7041cea9554c1d14667686d7d7c5dda83dd4f39e5dc4)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017611026763916016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 43,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0a72ce36164c2485a1e036d4239481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "\n",
    "from torch import nn\n",
    "from nltk import tokenize as nltk_tokenizer\n",
    "\n",
    "dataset = load_dataset(\"McGill-NLP/feedbackQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4626aded-95e1-47ec-bb47-59914940c8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rating_scores = {'Excellent':3 , 'Acceptable':2 , 'Could be Improved':1, 'Bad': -1}\n",
    "\n",
    "def process_df(df):\n",
    "    df['question'] = df['question'].apply(lambda x: x.replace('\\n',' '))\n",
    "    df['answer'] = df['answer'].apply(lambda x: x.replace('\\n',' '))\n",
    "    df['list_feedback'] = df['feedback'].apply(lambda x: [ r + \"___\" + e for r,e in zip(x['rating'],x['explanation']) ])\n",
    "    df['sampled_feedback'] = df['list_feedback'].apply(lambda x: np.random.choice(x).split(\"___\") )\n",
    "    df['rating_score'] = df['sampled_feedback'].apply(lambda x: rating_scores[x[0]])\n",
    "    df['rating'] = df['sampled_feedback'].apply(lambda x: x[0])\n",
    "    df['explanation'] = df['sampled_feedback'].apply(lambda x: x[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd2ffa3d-fae5-45d9-89c6-899dba3982fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = process_df(pd.DataFrame(dataset['train']))\n",
    "val_df = process_df(pd.DataFrame(dataset['validation']))\n",
    "test_df = process_df(pd.DataFrame(dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40dc46db-a864-459a-8e7a-b82df484643b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "bert_chkpt = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_chkpt)\n",
    "model = AutoModel.from_pretrained(bert_chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d31c623b-2b7b-4b28-a499-b63ba8311ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '[UNK]', '<pad>', '<mask>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "744c4863-8096-4c67-b473-90e03a0df39c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>feedback</th>\n",
       "      <th>list_feedback</th>\n",
       "      <th>sampled_feedback</th>\n",
       "      <th>rating_score</th>\n",
       "      <th>rating</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>Coronavirus (COVID-19) information for job see...</td>\n",
       "      <td>{'rating': ['Excellent', 'Could be Improved'],...</td>\n",
       "      <td>[Excellent___Has a link to detailed informatio...</td>\n",
       "      <td>[Could be Improved, This answer provides a lin...</td>\n",
       "      <td>1</td>\n",
       "      <td>Could be Improved</td>\n",
       "      <td>This answer provides a link for job searches, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>Coronavirus (COVID-19) information for job see...</td>\n",
       "      <td>{'rating': ['Excellent', 'Excellent'], 'explan...</td>\n",
       "      <td>[Excellent___A link to a job search website is...</td>\n",
       "      <td>[Excellent, A link to a job search website is ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>A link to a job search website is included, as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do I get help finding a job?</td>\n",
       "      <td>Coronavirus (COVID-19) information and support...</td>\n",
       "      <td>{'rating': ['Bad', 'Acceptable'], 'explanation...</td>\n",
       "      <td>[Bad___Talks about tax credits for businesses ...</td>\n",
       "      <td>[Bad, Talks about tax credits for businesses t...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Talks about tax credits for businesses that hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If I am in Australia on a worker holiday marke...</td>\n",
       "      <td>Frequently Asked Questions Working holiday mak...</td>\n",
       "      <td>{'rating': ['Could be Improved', 'Acceptable']...</td>\n",
       "      <td>[Could be Improved___Answer is about Working H...</td>\n",
       "      <td>[Acceptable, Answer is rather cut and dry but ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Acceptable</td>\n",
       "      <td>Answer is rather cut and dry but is also a lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If I am in Australia on a worker holiday marke...</td>\n",
       "      <td>Frequently Asked Questions COVID-19 Pandemic -...</td>\n",
       "      <td>{'rating': ['Bad', 'Could be Improved'], 'expl...</td>\n",
       "      <td>[Bad___Discusses pandemic visas. Doesn't menti...</td>\n",
       "      <td>[Bad, Discusses pandemic visas. Doesn't mentio...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Discusses pandemic visas. Doesn't mention the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                   How do I get help finding a job?   \n",
       "1                   How do I get help finding a job?   \n",
       "2                   How do I get help finding a job?   \n",
       "3  If I am in Australia on a worker holiday marke...   \n",
       "4  If I am in Australia on a worker holiday marke...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Coronavirus (COVID-19) information for job see...   \n",
       "1  Coronavirus (COVID-19) information for job see...   \n",
       "2  Coronavirus (COVID-19) information and support...   \n",
       "3  Frequently Asked Questions Working holiday mak...   \n",
       "4  Frequently Asked Questions COVID-19 Pandemic -...   \n",
       "\n",
       "                                            feedback  \\\n",
       "0  {'rating': ['Excellent', 'Could be Improved'],...   \n",
       "1  {'rating': ['Excellent', 'Excellent'], 'explan...   \n",
       "2  {'rating': ['Bad', 'Acceptable'], 'explanation...   \n",
       "3  {'rating': ['Could be Improved', 'Acceptable']...   \n",
       "4  {'rating': ['Bad', 'Could be Improved'], 'expl...   \n",
       "\n",
       "                                       list_feedback  \\\n",
       "0  [Excellent___Has a link to detailed informatio...   \n",
       "1  [Excellent___A link to a job search website is...   \n",
       "2  [Bad___Talks about tax credits for businesses ...   \n",
       "3  [Could be Improved___Answer is about Working H...   \n",
       "4  [Bad___Discusses pandemic visas. Doesn't menti...   \n",
       "\n",
       "                                    sampled_feedback  rating_score  \\\n",
       "0  [Could be Improved, This answer provides a lin...             1   \n",
       "1  [Excellent, A link to a job search website is ...             3   \n",
       "2  [Bad, Talks about tax credits for businesses t...            -1   \n",
       "3  [Acceptable, Answer is rather cut and dry but ...             2   \n",
       "4  [Bad, Discusses pandemic visas. Doesn't mentio...            -1   \n",
       "\n",
       "              rating                                        explanation  \n",
       "0  Could be Improved  This answer provides a link for job searches, ...  \n",
       "1          Excellent  A link to a job search website is included, as...  \n",
       "2                Bad  Talks about tax credits for businesses that hi...  \n",
       "3         Acceptable  Answer is rather cut and dry but is also a lit...  \n",
       "4                Bad  Discusses pandemic visas. Doesn't mention the ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9078f4a5-10f0-4d44-886e-53ae3826ea30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coronavirus (COVID-19) information for job seekers Exisiting job seekers If you are a current job seeker or participant, this fact sheet provides important information about mutual obligation requirements, appointments with your provider, and what to do if you are self-isolating:  Information for job seekers and participants  If you are participating in the ParentsNext program, this fact sheet provides important information about your activities and appointments.   Information for ParentsNext participants   ParentsNext participants Frequently Asked Questions   If you are a New Business Assistance with NEIS participant, these Frequently Asked Questions (FAQ) provides information about accessing the Coronavirus Supplement and what support is available during this time:  New Business Assistance with NEIS participants - Frequently Asked Questions  If you are a New Business Assistance with NEIS provider, these Frequently Asked Questions (FAQ) provides information about supporting NEIS participants during the Coronavirus situation.  New Business Assistance with NEIS providers – Frequently Asked Questions  *[NEIS]: New Enterprise Incentive Scheme'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['answer'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "619de540-d154-431a-8d5e-bab3bba6d7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  7596,  1014,  2133,  2028,  2021,  2729,  1033,     2, 19614,\n",
       "          4139,  9545,  9545,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'length': tensor([14])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Hello, how are you doing?'+ f\" {tokenizer.eos_token} \" + \"Hemlooooo\",add_special_tokens=True,return_tensors='pt', return_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af181ac2-b1ae-4fcd-b59d-74b4defb3d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_tokenizer.sent_tokenize(train_df['answer'].loc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f22dd1b8-d28f-4d16-9acc-cd3dce4579cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[21891, 23354, 1010, 2526, 17262, 1015, 2543, 1011, 2596, 2009, 3109, 24075, 4658, 17421, 3440, 3109, 24075, 2069, 2021, 2028, 1041, 2787, 3109, 29448, 2034, 13184, 1014, 2027, 2759, 7127, 3644, 2594, 2596, 2059, 8207, 14991, 5922, 1014, 14655, 2011, 2119, 10806, 1014, 2002, 2058, 2004, 2083, 2069, 2021, 2028, 2973, 1015, 11167, 22252, 1028, 2596, 2009, 3109, 24075, 2002, 6822, 2069, 2021, 2028, 8023, 2003, 2000, 3012, 2642, 18417, 2569, 1014, 2027, 2759, 7127, 3644, 2594, 2596, 2059, 2119, 3454, 2002, 14655, 1016], [2596, 2009, 3012, 2642, 18417, 6822, 3012, 2642, 18417, 6822, 4707, 2360, 3984, 2069, 2021, 2028, 1041, 2051, 2453, 5379, 2011, 11269, 2487, 13184, 1014, 2126, 4707, 2360, 3984, 1010, 6908, 4164, 1011, 3644, 2596, 2059, 3233, 2079, 2000, 21891, 23354, 12452, 2002, 2058, 2494, 2007, 2804, 2080, 2027, 2055, 1028, 2051, 2453, 5379, 2011, 11269, 2487, 6822, 1015, 4707, 2360, 3984, 2069, 2021, 2028, 1041, 2051, 2453, 5379, 2011, 11269, 2487, 10806, 1014, 2126, 4707, 2360, 3984, 1010, 6908, 4164, 1011, 3644, 2596, 2059, 4641, 11269, 2487, 6822, 2080, 2000, 21891, 23354, 3667, 1016], [2051, 2453, 5379, 2011, 11269, 2487, 11674, 1520, 4707, 2360, 3984, 1012, 1035, 11269, 2487, 1037, 1028, 2051, 6964, 20442, 5683]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_inp = tokenizer(nltk_tokenizer.sent_tokenize(train_df['answer'].loc[0]),add_special_tokens=False,return_token_type_ids=True)#,max_length=200,padding='max_length')\n",
    "tok_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "05e6d281-2513-459b-a854-3f099d26309b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Hello, how are you?',return_special_tokens_mask=True,add_special_tokens=True, padding='max_length', max_length=20)['special_tokens_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a093a991-7536-4082-9bd2-7602825edfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class feedback_QA_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,df,max_length=512):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.data = []\n",
    "        skipped = 0\n",
    "        \n",
    "        for i in tqdm.tqdm(range(len(self.df)),desc='vectorizing..'):\n",
    "            \n",
    "            d = {}\n",
    "            \n",
    "            tok_question = tokenizer('Question: ' + self.df.iloc[i]['question'] + ' Answer: ', add_special_tokens=False)\n",
    "            tok_answer = tokenizer(self.df.iloc[i]['answer'], add_special_tokens=False)\n",
    "            tok_feedback = tokenizer(self.df.iloc[i]['explanation'], add_special_tokens=False)\n",
    "            \n",
    "            if len(tok_question['input_ids']+tok_answer['input_ids']+tok_feedback['input_ids'])+4 > self.max_len:\n",
    "                skipped +=1\n",
    "                continue\n",
    "            \n",
    "            context = [tokenizer.bos_token_id] + tok_question['input_ids'] + tok_answer['input_ids']\n",
    "            context_attn = [1] + tok_question['attention_mask'] + tok_answer['attention_mask']\n",
    "            context_pool_mask = [0] + [0]*len(tok_question['input_ids']) + tok_answer['attention_mask']\n",
    "            \n",
    "            \n",
    "            d['context_w_feedback'] = context + [tokenizer.sep_token_id]*2 + tok_feedback['input_ids'] + [tokenizer.eos_token_id]\n",
    "            \n",
    "            PAD_LEN = self.max_len - len(d['context_w_feedback'])\n",
    "            \n",
    "            d['context_w_feedback'] += [tokenizer.pad_token_id]*PAD_LEN\n",
    "            d['context_w_feedback_attn'] = context_attn + [1,1] + tok_feedback['attention_mask'] + [1] + [0]*PAD_LEN            \n",
    "            d['context'] = d['context_w_feedback']\n",
    "            d['context_attn'] = context_attn + [1,0] + [0]*len(tok_feedback['attention_mask']) + [0] + [0]*PAD_LEN\n",
    "            \n",
    "            d['feedback_pool_mask'] = [0]*len(context_pool_mask) + [0,0] + tok_feedback['attention_mask'] + [0] + [0]*PAD_LEN\n",
    "            d['answer_pool_mask'] = context_pool_mask + [0,0] + [0]*len(tok_feedback['attention_mask']) + [0] + [0]*PAD_LEN\n",
    "            \n",
    "            answer_phrases = nltk_tokenizer.sent_tokenize(self.df.iloc[i]['answer'])\n",
    "            tok_phrases = tokenizer(answer_phrases,add_special_tokens=False,return_token_type_ids=True)\n",
    "            \n",
    "            d['answer_phrases_pool_mask'] = []\n",
    "            \n",
    "            for j in range(len(answer_phrases)):\n",
    "                answer_phrases_attn_mask = tok_phrases['token_type_ids'].copy()\n",
    "                answer_phrases_attn_mask[j] = tok_phrases['attention_mask'][j].copy()\n",
    "                answer_phrases_attn_mask = list(itertools.chain.from_iterable(answer_phrases_attn_mask))\n",
    "                # pad_len = len(tok_answer['attention_mask']) - len(answer_phrases_attn_mask)\n",
    "                # answer_phrases_attn_mask += [0]*pad_len\n",
    "                \n",
    "                answer_phrase_pool_mask = [0] + [0]*len(tok_question['input_ids']) + answer_phrases_attn_mask + [0,0] + [0]*len(tok_feedback['attention_mask']) + [0] + [0]*PAD_LEN\n",
    "                \n",
    "                d['answer_phrases_pool_mask'].append(answer_phrase_pool_mask)\n",
    "            \n",
    "            if len(d['answer_phrases_pool_mask'][0])>len(d['answer_pool_mask']):\n",
    "                skipped +=1\n",
    "                continue\n",
    "                \n",
    "            self.data.append(d)\n",
    "        print('Skipped: ',skipped)\n",
    "\n",
    "    def add_neg_samples(self):\n",
    "        for i in tqdm.tqdm(range(self.__len__()),desc='adding neg samples...'):\n",
    "            self.data[i]['feedback_set'] = [self.data[i]['context_w_feedback']]\n",
    "            self.data[i]['feedback_attn_set'] = [self.data[i]['context_w_feedback_attn']]\n",
    "            self.data[i]['feedback_pool_mask_set'] = [self.data[i]['feedback_pool_mask']]\n",
    "            L = list(range(self.__len__()))\n",
    "            L.remove(i)\n",
    "            neg_samples_idx = np.random.choice(L,size=4)\n",
    "            for n_id in neg_samples_idx:\n",
    "                self.data[i]['feedback_set'].append(self.data[n_id]['context_w_feedback'])\n",
    "                self.data[i]['feedback_attn_set'].append(self.data[n_id]['context_w_feedback_attn'])\n",
    "                self.data[i]['feedback_pool_mask_set'].append(self.data[n_id]['feedback_pool_mask'])\n",
    "            for k in self.data[i].keys():\n",
    "                self.data[i][k] = torch.tensor(self.data[i][k])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c69c540d-1963-419c-8c09-08f5d65aa803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vectorizing..: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5660/5660 [00:17<00:00, 315.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped:  436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding neg samples...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 5224/5224 [00:21<00:00, 242.14it/s]\n",
      "vectorizing..: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1410/1410 [00:04<00:00, 325.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped:  105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding neg samples...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1305/1305 [00:04<00:00, 273.21it/s]\n",
      "vectorizing..: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1995/1995 [00:06<00:00, 317.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped:  174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding neg samples...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1821/1821 [00:07<00:00, 253.42it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = feedback_QA_dataset(train_df)\n",
    "train_dataset.add_neg_samples()\n",
    "valid_dataset = feedback_QA_dataset(val_df)\n",
    "valid_dataset.add_neg_samples()\n",
    "test_dataset = feedback_QA_dataset(test_df)\n",
    "test_dataset.add_neg_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92049336-0c27-4e1d-b5f7-9791745c5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DL = DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "valid_DL = DataLoader(valid_dataset,batch_size=1,shuffle=True)\n",
    "test_DL = DataLoader(test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1da63016-24f0-400e-9a29-9093ab832bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_w_feedback torch.Size([1, 512])\n",
      "context_w_feedback_attn torch.Size([1, 512])\n",
      "context torch.Size([1, 512])\n",
      "context_attn torch.Size([1, 512])\n",
      "feedback_pool_mask torch.Size([1, 512])\n",
      "answer_pool_mask torch.Size([1, 512])\n",
      "answer_phrases_pool_mask torch.Size([1, 11, 512])\n",
      "feedback_set torch.Size([1, 5, 512])\n",
      "feedback_attn_set torch.Size([1, 5, 512])\n",
      "feedback_pool_mask_set torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "for b in train_DL:\n",
    "    for k in b.keys():\n",
    "        print(k,b[k].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2af4587b-45e7-40cb-962d-faec921133ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3]), torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[[1,2,3],[4,5,6]]])\n",
    "t.shape,t.repeat(2,1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "524eef71-2aad-4803-892f-01b0a6fa82de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 768]) torch.Size([1, 768]) torch.Size([2, 768]) tensor([0.6895, 0.3283, 0.2529, 0.2336, 0.3751], device='cuda:0') tensor([0.6700, 0.3212, 0.2468, 0.2296, 0.3667], device='cuda:0')\n",
      "softmax:  tensor([0.2698, 0.1880, 0.1743, 0.1710, 0.1970], device='cuda:0') tensor([[0.2579, 0.1916, 0.1736, 0.1766, 0.2003],\n",
      "        [0.2766, 0.1854, 0.1763, 0.1675, 0.1942]], device='cuda:0')\n",
      "CE Loss:  tensor(1.3102, device='cuda:0') tensor(1.3198, device='cuda:0')\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raja/Ravi/rghadia_env/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 768]) torch.Size([1, 768]) torch.Size([2, 768]) tensor([0.6603, 0.2329, 0.2974, 0.2593, 0.1634], device='cuda:0') tensor([0.6352, 0.2273, 0.2901, 0.2499, 0.1616], device='cuda:0')\n",
      "softmax:  tensor([0.2758, 0.1799, 0.1919, 0.1847, 0.1678], device='cuda:0') tensor([[0.2620, 0.1842, 0.1982, 0.1800, 0.1756],\n",
      "        [0.2822, 0.1775, 0.1870, 0.1901, 0.1632]], device='cuda:0')\n",
      "CE Loss:  tensor(1.2881, device='cuda:0') tensor(1.3018, device='cuda:0')\n",
      "----------------------------\n",
      "torch.Size([5, 768]) torch.Size([1, 768]) torch.Size([6, 768]) tensor([0.8366, 0.6493, 0.8126, 0.2180, 0.1444], device='cuda:0') tensor([0.8052, 0.6229, 0.7825, 0.2107, 0.1363], device='cuda:0')\n",
      "softmax:  tensor([0.2601, 0.2157, 0.2539, 0.1401, 0.1302], device='cuda:0') tensor([[0.2623, 0.2151, 0.2516, 0.1408, 0.1302],\n",
      "        [0.2608, 0.2125, 0.2557, 0.1447, 0.1263],\n",
      "        [0.2564, 0.2148, 0.2568, 0.1435, 0.1285],\n",
      "        [0.2532, 0.2189, 0.2490, 0.1431, 0.1357],\n",
      "        [0.2568, 0.2137, 0.2518, 0.1406, 0.1371],\n",
      "        [0.2588, 0.2153, 0.2487, 0.1416, 0.1356]], device='cuda:0')\n",
      "CE Loss:  tensor(1.3467, device='cuda:0') tensor(1.3545, device='cuda:0')\n",
      "----------------------------\n",
      "torch.Size([5, 768]) torch.Size([1, 768]) torch.Size([4, 768]) tensor([0.9087, 0.1580, 0.2068, 0.1786, 0.0782], device='cuda:0') tensor([0.8998, 0.1568, 0.2042, 0.1783, 0.0792], device='cuda:0')\n",
      "softmax:  tensor([0.3466, 0.1636, 0.1718, 0.1670, 0.1510], device='cuda:0') tensor([[0.3439, 0.1642, 0.1733, 0.1689, 0.1497],\n",
      "        [0.3479, 0.1629, 0.1705, 0.1662, 0.1524],\n",
      "        [0.3440, 0.1647, 0.1732, 0.1665, 0.1517],\n",
      "        [0.3431, 0.1642, 0.1708, 0.1686, 0.1532]], device='cuda:0')\n",
      "CE Loss:  tensor(1.0596, device='cuda:0') tensor(1.0649, device='cuda:0')\n",
      "----------------------------\n",
      "torch.Size([5, 768]) torch.Size([1, 768]) torch.Size([15, 768]) tensor([ 0.4861,  0.2393,  0.1666,  0.0957, -0.0285], device='cuda:0') tensor([ 0.4714,  0.2280,  0.1552,  0.0879, -0.0310], device='cuda:0')\n",
      "softmax:  tensor([0.2644, 0.2066, 0.1921, 0.1789, 0.1580], device='cuda:0') tensor([[0.2606, 0.2003, 0.1953, 0.1847, 0.1591],\n",
      "        [0.2627, 0.2016, 0.1948, 0.1850, 0.1559],\n",
      "        [0.2587, 0.2037, 0.1970, 0.1819, 0.1587],\n",
      "        [0.2649, 0.2060, 0.1911, 0.1794, 0.1585],\n",
      "        [0.2582, 0.2109, 0.1937, 0.1779, 0.1593],\n",
      "        [0.2633, 0.2148, 0.1868, 0.1754, 0.1597],\n",
      "        [0.2669, 0.2069, 0.1899, 0.1772, 0.1591],\n",
      "        [0.2661, 0.2116, 0.1908, 0.1752, 0.1564],\n",
      "        [0.2658, 0.2073, 0.1895, 0.1793, 0.1582],\n",
      "        [0.2642, 0.2063, 0.1910, 0.1793, 0.1592],\n",
      "        [0.2628, 0.2096, 0.1900, 0.1779, 0.1597],\n",
      "        [0.2619, 0.2003, 0.1935, 0.1800, 0.1643],\n",
      "        [0.2588, 0.2037, 0.1947, 0.1796, 0.1633],\n",
      "        [0.2654, 0.2071, 0.1913, 0.1787, 0.1574],\n",
      "        [0.2678, 0.2050, 0.1884, 0.1786, 0.1602]], device='cuda:0')\n",
      "CE Loss:  tensor(1.3303, device='cuda:0') tensor(1.3348, device='cuda:0')\n",
      "----------------------------\n",
      "torch.Size([5, 768]) torch.Size([1, 768]) torch.Size([3, 768]) tensor([0.8324, 0.2005, 0.2133, 0.5215, 0.1381], device='cuda:0') tensor([0.7898, 0.1958, 0.1989, 0.5067, 0.1206], device='cuda:0')\n",
      "softmax:  tensor([0.3028, 0.1610, 0.1630, 0.2219, 0.1512], device='cuda:0') tensor([[0.3073, 0.1582, 0.1644, 0.2143, 0.1558],\n",
      "        [0.2889, 0.1700, 0.1651, 0.2235, 0.1525],\n",
      "        [0.2937, 0.1632, 0.1631, 0.2328, 0.1473]], device='cuda:0')\n",
      "CE Loss:  tensor(1.1946, device='cuda:0') tensor(1.2153, device='cuda:0')\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = AutoModel.from_pretrained(bert_chkpt).to(device)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    se = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return F.normalize(se, p=2, dim=1)\n",
    "\n",
    "j = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b in test_DL:\n",
    "        se = mean_pooling( model(input_ids = b['context_w_feedback'].to(device),attention_mask=b['context_attn'].to(device))[0], b['answer_pool_mask'].to(device))\n",
    "        \n",
    "        fmo = model(input_ids = b['feedback_set'][0].to(device),attention_mask=b['feedback_attn_set'][0].to(device))[0]\n",
    "        # print(b['feedback_set'][0].shape, b['feedback_attn_set'][0].shape, fmo.shape, b['feedback_pool_mask_set'][0].shape)\n",
    "        \n",
    "        fe = mean_pooling(model(input_ids = b['feedback_set'][0].to(device),attention_mask=b['feedback_attn_set'][0].to(device))[0], b['feedback_pool_mask_set'][0].to(device))\n",
    "        \n",
    "        pmo = model(input_ids = b['context_w_feedback'].to(device),attention_mask=b['context_attn'].to(device))\n",
    "        # print(pmo[0].shape,b['answer_phrases_pool_mask'].shape,pmo[0].repeat(b['answer_phrases_pool_mask'][0].shape[0],1,1).shape,b['answer_phrases_pool_mask'][0].shape)\n",
    "        pe = mean_pooling(pmo[0].repeat(b['answer_phrases_pool_mask'][0].shape[0],1,1),b['answer_phrases_pool_mask'][0].to(device) )# for i in range(b['answer_phrases_pool_mask'][0].shape[0])]\n",
    "        # pe = torch.stack(pe).squeeze(1)\n",
    "        cos_sim = F.cosine_similarity(se,fe,dim=1)\n",
    "        cos_phrase_sim = torch.matmul(pe,fe.transpose(1,0))\n",
    "        print(fe.shape,se.shape,pe.shape,cos_sim,cos_phrase_sim.mean(0))\n",
    "        \n",
    "        sent_probs = F.softmax(cos_sim,dim=-1)\n",
    "        phrase_probs = F.softmax(cos_phrase_sim,dim=-1)\n",
    "        \n",
    "        # print('\\nInput: ',tokenizer.decode(torch.mul(b['context_w_feedback'][0],b['context_attn'][0]),skip_special_tokens=True),'\\n')\n",
    "        # print('Feedback: ',tokenizer.decode(torch.mul(b['context_w_feedback'][0],b['feedback_pool_mask'][0]),skip_special_tokens=True),'\\n')\n",
    "#         for i in range(b['answer_phrases_pool_mask'][0].shape[0]):\n",
    "#             relevance = phrase_probs[i][0] - sent_probs[0]\n",
    "            \n",
    "#             phrase_tok = torch.mul(b['context_w_feedback'][0],b['answer_phrases_pool_mask'][0][i])\n",
    "#             print(f\"Phrase {i}:\",tokenizer.decode(phrase_tok,skip_special_tokens=True))\n",
    "#             print(f\"Relevance of phrase {i} is {relevance}\",'\\n')\n",
    "        \n",
    "        print('softmax: ',F.softmax(cos_sim),F.softmax(cos_phrase_sim,dim=-1))\n",
    "        \n",
    "        tgt_tensor = torch.zeros(b['feedback_set'].shape[1] , device=device)\n",
    "        tgt_tensor[0] = 1.0\n",
    "        print('CE Loss: ', F.cross_entropy(cos_sim,target=tgt_tensor), F.cross_entropy(cos_phrase_sim.mean(0),target=torch.tensor([1.0,0,0,0,0]).to(device)))\n",
    "        print('----------------------------')\n",
    "        j+=1\n",
    "        if j>5:\n",
    "            break\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0f3ed8bc-1d6e-47fa-bfbb-0f6b3b1d97b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4, 5],\n",
       "         [6, 7, 8, 9, 0]],\n",
       "\n",
       "        [[1, 2, 3, 4, 5],\n",
       "         [6, 7, 8, 9, 0]]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[[1,2,3,4,5],[6,7,8,9,0]]])\n",
    "t.repeat(2,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "39373ab0-572f-42ec-8bee-d6945dd568bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    def __init__(self, model_chkpt, device='cuda:0'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = AutoModel.from_pretrained(model_chkpt).to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def mean_pooling(self,model_output,attention_mask):\n",
    "        token_embeddings = model_output #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        se = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return F.normalize(se, p=2, dim=1)\n",
    "        \n",
    "    def forward(self, b):\n",
    "        sent_model_out = self.model(input_ids = b['context_w_feedback'].to(self.device),attention_mask=b['context_attn'].to(self.device))[0]\n",
    "        feedback_model_out = self.model(input_ids = b['feedback_set'][0].to(self.device),attention_mask=b['feedback_attn_set'][0].to(self.device))[0]\n",
    "        \n",
    "        sent_emb = self.mean_pooling( sent_model_out, b['answer_pool_mask'].to(self.device))\n",
    "        feedback_emb = self.mean_pooling( feedback_model_out, b['feedback_pool_mask_set'][0].to(self.device))\n",
    "        \n",
    "        # print(pmo[0].shape,b['answer_phrases_pool_mask'].shape)\n",
    "        phrase_emb = self.mean_pooling( sent_model_out.repeat(b['answer_phrases_pool_mask'][0].shape[0],1,1), b['answer_phrases_pool_mask'][0].to(self.device) )\n",
    "        # phrase_emb = torch.stack(phrase_emb).squeeze(1)\n",
    "        cos_sim = F.cosine_similarity(sent_emb,feedback_emb,dim=1)\n",
    "        cos_phrase_sim = torch.matmul(phrase_emb,feedback_emb.transpose(1,0))\n",
    "        \n",
    "        tgt_tensor = torch.zeros(b['feedback_set'].shape[1] , device=self.device)\n",
    "        tgt_tensor[0] = 1.0 #the relevant feedback is always present at index 0\n",
    "        \n",
    "        return_dict = {'sent_ce_loss': F.cross_entropy(cos_sim,target=tgt_tensor),\n",
    "                       'avg_phrase_ce_loss': F.cross_entropy(cos_phrase_sim.mean(0),target=tgt_tensor),\n",
    "                       'sent_probs': F.softmax(cos_sim,dim=-1),\n",
    "                       'phrase_probs': F.softmax(cos_phrase_sim,dim=-1)}\n",
    "        \n",
    "        return return_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "97a1fcd9-7fb7-4557-a1ee-023591930420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(discriminator,train_dl,valid_dl,epochs,batch_size,optimizer,PATIENCE=20,save_dir=None):\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    \n",
    "    discriminator.train()\n",
    "    \n",
    "    loss_acc = 0\n",
    "    num_batches = 0\n",
    "    total_steps = 0\n",
    "    best_valid_loss = np.inf\n",
    "    patience = PATIENCE\n",
    "    \n",
    "    train_loss_arr,valid_loss_arr = [],[]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    discriminator.zero_grad()\n",
    "    \n",
    "    valid_loss = validate(discriminator,valid_dl)\n",
    "    valid_loss_arr.append(valid_loss/len(valid_dl))\n",
    "    \n",
    "    for E in range(epochs):\n",
    "        \n",
    "        num_samples = 0\n",
    "        \n",
    "        for b in train_dl:\n",
    "            \n",
    "            y = discriminator(b)\n",
    "                          # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                          # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "            loss = y['sent_ce_loss'] + y['avg_phrase_ce_loss'] #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "            \n",
    "            num_samples+=1\n",
    "            \n",
    "            loss.backward()\n",
    "            loss_acc += loss.item()\n",
    "            \n",
    "            if num_samples%batch_size==0:\n",
    "                optimizer.step()\n",
    "\n",
    "                num_batches += 1\n",
    "                total_steps += 1\n",
    "            \n",
    "                train_loss_arr.append(loss_acc/num_batches)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                if total_steps%100==0 and total_steps!=0:\n",
    "                    print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "            \n",
    "        #print(\"Epoch:\",E,\"\\t\",\"Steps taken:\",total_steps,\"\\tLoss:\",loss_acc/num_batches)\n",
    "        \n",
    "        torch.save({'model_state':discriminator.state_dict(),\n",
    "                    'optimizer':optimizer.state_dict(),\n",
    "                    'epoch':E},\n",
    "                    f\"{save_dir}/Epoch_{E}_model_chkpt.pth.tar\")\n",
    "        \n",
    "        valid_loss = validate(discriminator,valid_dl)\n",
    "        valid_loss_arr.append(valid_loss/len(valid_dl))\n",
    "        \n",
    "        if valid_loss<best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience = PATIENCE\n",
    "            \n",
    "            torch.save({'model_state':discriminator.state_dict(),\n",
    "                        'optimizer':optimizer.state_dict(),\n",
    "                        'epoch':E},\n",
    "                        f\"{save_dir}/best_model_chkpt.pth.tar\")\n",
    "        else:\n",
    "            patience -= 1\n",
    "            print(f\"REDUCING PATIENCE...{patience}\")\n",
    "\n",
    "        if patience<=0:\n",
    "            print(\"RUNNING OUT OF PATIENCE... TERMINATING\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return train_loss_arr,valid_loss_arr\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b5e5931d-65fc-421d-8bf3-4bdb58b8b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(discriminator,valid_dl):\n",
    "    \n",
    "    discriminator.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for b in valid_dl:\n",
    "            y = discriminator(b)\n",
    "                          # decoder_input_ids=b['feedback'].squeeze(1)[:,:-1].to(device),\n",
    "                          # decoder_attention_mask=b['feedback_attn'].squeeze(1)[:,:-1].to(device))\n",
    "            loss = y['sent_ce_loss'] + y['avg_phrase_ce_loss'] #F.cross_entropy(y.logits.permute(0,2,1), b['feedback'].squeeze(1)[:,1:].to(device), ignore_index=tokenizer.pad_token_id)\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    print(\"Validation Loss:\",valid_loss)\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7d1cb84b-dd2e-495f-845d-03ec726b7bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3223.12977540493\n",
      "Epoch: 0 \t Steps taken: 100 \tLoss: 29.962738287448882\n",
      "Epoch: 0 \t Steps taken: 200 \tLoss: 29.572392408251762\n",
      "Epoch: 0 \t Steps taken: 300 \tLoss: 29.432847184737522\n",
      "Validation Loss: 2379.9055646657944\n",
      "Epoch: 1 \t Steps taken: 400 \tLoss: 29.339444583654405\n",
      "Epoch: 1 \t Steps taken: 500 \tLoss: 29.240687273740768\n",
      "Epoch: 1 \t Steps taken: 600 \tLoss: 29.17627572039763\n",
      "Validation Loss: 2379.1063376665115\n",
      "Epoch: 2 \t Steps taken: 700 \tLoss: 29.125879089662007\n",
      "Epoch: 2 \t Steps taken: 800 \tLoss: 29.04649720802903\n",
      "Epoch: 2 \t Steps taken: 900 \tLoss: 28.987862569358615\n",
      "Validation Loss: 2380.6113728284836\n",
      "Epoch: 3 \t Steps taken: 1000 \tLoss: 28.943698433041572\n",
      "Epoch: 3 \t Steps taken: 1100 \tLoss: 28.865703828659925\n",
      "Epoch: 3 \t Steps taken: 1200 \tLoss: 28.807999064425626\n",
      "Epoch: 3 \t Steps taken: 1300 \tLoss: 28.75654564353136\n",
      "REDUCING PATIENCE...4\n",
      "Validation Loss: 2387.3019028902054\n",
      "Epoch: 4 \t Steps taken: 1400 \tLoss: 28.691443885564805\n",
      "Epoch: 4 \t Steps taken: 1500 \tLoss: 28.62533722082774\n",
      "Epoch: 4 \t Steps taken: 1600 \tLoss: 28.570833625122905\n",
      "REDUCING PATIENCE...3\n",
      "Validation Loss: 2385.651179790497\n",
      "Epoch: 5 \t Steps taken: 1700 \tLoss: 28.507951061375\n",
      "Epoch: 5 \t Steps taken: 1800 \tLoss: 28.436860036585067\n",
      "Epoch: 5 \t Steps taken: 1900 \tLoss: 28.378810989668494\n",
      "REDUCING PATIENCE...2\n",
      "Validation Loss: 2391.657604575157\n",
      "Epoch: 6 \t Steps taken: 2000 \tLoss: 28.31956906890869\n",
      "Epoch: 6 \t Steps taken: 2100 \tLoss: 28.243385137489863\n",
      "Epoch: 6 \t Steps taken: 2200 \tLoss: 28.178970901857724\n",
      "REDUCING PATIENCE...1\n",
      "Validation Loss: 2404.951407432556\n",
      "Epoch: 7 \t Steps taken: 2300 \tLoss: 28.124763102997903\n",
      "Epoch: 7 \t Steps taken: 2400 \tLoss: 28.042747377107542\n",
      "Epoch: 7 \t Steps taken: 2500 \tLoss: 27.970856111049653\n",
      "Epoch: 7 \t Steps taken: 2600 \tLoss: 27.910075391301742\n",
      "REDUCING PATIENCE...0\n",
      "RUNNING OUT OF PATIENCE... TERMINATING\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "# MPNet = AutoModel.from_pretrained(bert_chkpt).to(device)\n",
    "discriminator_model = discriminator(bert_chkpt,device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(discriminator_model.parameters(),lr=1e-5)\n",
    "\n",
    "save_dir = 'Detect_Span_FB_MPNET_chkpts_3'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "train_loss,valid_loss = train(discriminator_model,\n",
    "                              train_DL,\n",
    "                              valid_DL,\n",
    "                              EPOCHS,\n",
    "                              BATCH_SIZE,\n",
    "                              optimizer,\n",
    "                              PATIENCE=5,\n",
    "                              save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91101fb6-ccc5-4b70-a781-f80306d79560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train_loss.json','w') as f:\n",
    "    json.dump(train_loss,f)\n",
    "\n",
    "with open('valid_loss.json','w') as f:\n",
    "    json.dump(valid_loss,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1438eb8-e8f1-41bd-ab9c-8c0ee15ab891",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ds = np.array(train_loss)[np.round(np.linspace(0, len(train_loss) - 1, len(valid_loss))).astype(int)]\n",
    "loss_df = pd.DataFrame({'train_loss':train_loss_ds , 'valid_loss':valid_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32161e3-1a96-4893-ac8a-5b567044334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import express as px\n",
    "px.line(loss_df,y=['train_loss','valid_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b9d00-0197-4bcc-be96-83e62d9c96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.load_state_dict(torch.load('GenFB_BART_chkpts_1/Epoch_0_model_chkpt.pth.tar')['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3a035-e5a3-49e4-bbf4-6a0699945e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for b in train_DL:\n",
    "    out = discriminator.generate(inputs=b['input'][0:1,0].to(device),top_p=0.5)\n",
    "    print(tokenizer.decode(b['input'][0:1,0][0],skip_special_tokens=True))\n",
    "    print(tokenizer.decode(b['feedback'][0:1,0][0],skip_special_tokens=True))\n",
    "    print(tokenizer.decode(out[0]))\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    i+=1\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96357d85-64e1-4d7e-8eab-ee36d405bbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
